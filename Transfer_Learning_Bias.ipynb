{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transfer_Learning_Bias.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNgszEM5pB4Aa4ws3UydZJZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2nobpo4_9yTy"},"source":["# !git clone https://github.com/shivangi1299/capstone.git\n","!git clone https://github.com/prosis369/capstone.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sE6nf2I__bN4"},"source":["%cd capstone"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnkpDJDd98zh"},"source":["!pip3 install pytorch_pretrained_bert\n","!pip3 install numpy\n","!pip3 install nltk\n","!pip3 install torch\n","!pip3 install pandas\n","!pip3 install sklearn\n","!pip3 install matplotlib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uUhNKW0c9_Ny"},"source":["!pip3 install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"laLRtxWe-AIB"},"source":["!pip3 install sklearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUtRkBiH-FqR"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVM3pGSvBwu9"},"source":["To run a pytorch bias model - need not run this for training transfer learning model"]},{"cell_type":"code","metadata":{"id":"nM3p7PeMBvNp"},"source":["!python3 transfer_bias.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v81nTnsgB_iB"},"source":["TRANSFER LEARNING FOR BIAS BEGINS"]},{"cell_type":"markdown","metadata":{"id":"3G7k2qrnNnm_"},"source":["Toxic Dataset"]},{"cell_type":"code","metadata":{"id":"d8sBY8y-B2rl","executionInfo":{"status":"ok","timestamp":1619621523076,"user_tz":-330,"elapsed":1170,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","from utils import np2autograd\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","from utils import max_sentence_size\n","from utils import avg_cross_entropy_loss\n","from utils import batch_generator_bias\n","from sklearn.metrics import accuracy_score"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"zD6Yva5HCYnl","executionInfo":{"status":"ok","timestamp":1619621525791,"user_tz":-330,"elapsed":1162,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["# Hyperparams\n","postag_reg = 1e-3\n","chunking_reg = 1e-3\n","learning_rate = 1e-3\n","postag_hn_size = 100\n","postag_nb_layers = 2\n","embedding_size = 512\n","bias_reg = 1e-3\n","nb_postags = 1"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"BPjyDq1gCZYb","executionInfo":{"status":"ok","timestamp":1619621527921,"user_tz":-330,"elapsed":1901,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["class TransferBiasClassification(nn.Module):\n","  \n","    def __init__(self):\n","        super(TransferBiasClassification, self).__init__()\n","\n","        self.w = nn.Parameter(torch.randn(postag_nb_layers * 2,\n","                                          max_sentence_size,\n","                                          postag_hn_size))\n","        self.h = nn.Parameter(torch.randn(postag_nb_layers * 2,\n","                                          max_sentence_size,\n","                                          postag_hn_size))\n","\n","        # Bidirectional LSTM\n","        self.bi_lstm = nn.LSTM(embedding_size,\n","                               postag_hn_size,\n","                               postag_nb_layers,\n","                               bidirectional=True)\n","\n","        self.fc = nn.Linear(postag_hn_size * 2, nb_postags)\n","\n","    def forward(self, x):\n","        # Runs the LSTM for each word-vector in the sentence x\n","        # x = [x]\n","        x = torch.tensor(x, dtype=torch.float32)\n","        out, hn = self.bi_lstm(x, (self.h[:, :x.size(1), :],\n","                                   self.w[:, :x.size(1), :]))\n","\n","        # Runs a linear classifier on the outputed state vector\n","        tags = self.fc(out[0])\n","\n","        return tags\n","    \n","    def bias_loss(self, y, yt):\n","        loss = (yt.float() - y) ** 2 \\\n","               + (transfer_bias_model.w.norm() ** 2) * bias_reg\n","\n","        return loss\n","\n","    def loss(self, y, bias):\n","        # print(\"Hi i am in loss\")\n","        losses = []\n","        # print(\"Hi i am in loss line 2\")\n","        length = len(y)\n","\n","        for i in range(length):\n","\n","            p_bias, r_bias = y[i][0], np2autograd(bias[i])\n","            loss_bias = self.bias_loss(p_bias, r_bias)\n","\n","            losses.append(loss_bias)\n","\n","        loss = losses[0]\n","\n","        for i in range(1, length):\n","            loss += losses[i]\n","\n","        loss = loss / length\n","\n","        return loss\n","\n","\n","def threshold(prediction, upperBound, lowerBound):\n","  # print(type(prediction))\n","  train_predictions.append(prediction)\n","  if prediction >= upperBound:\n","      prediction = 0\n","  elif prediction < lowerBound:\n","      prediction = 0\n","  else:\n","      prediction = 1\n","  return prediction\n","\n","def compare(out):\n","\n","    # print(out[0][0].numpy())\n","    predicted_bias = out[0][0].numpy()\n","    predicted_bias = threshold(predicted_bias, 0.2, 0.1)\n","    \n","    return [predicted_bias]\n","\n","def accuracy(train_batch_acc, bias_nb_batches):\n","\n","  pred_bias = []\n","\n","  l = len(train_batch_acc)\n","\n","  for i in range(l):\n","    pred_bias.append(train_batch_acc[i][0])\n","\n","  print(pred_bias)\n","  print(bias_nb_batches)\n","\n","  bias_acc = accuracy_score(bias_nb_batches, pred_bias)\n","  \n","  return(bias_acc)\n","\n","def mapPrediction(predicted_bias, bias_nb_batches):\n","  # print(type(predicted_bias))\n","  # predicted_bias = predicted_bias.astype(np.float)\n","  # print(predicted_bias)\n","  actual_0 = []\n","  actual_1 = []\n","\n","  for i in range(len(predicted_bias)):\n","\n","    # print(type(predicted_bias[i]))\n","    # print(type(bias_nb_batches[i]))\n","    if len(actual_0)==0 and len(actual_1)==0:\n","      if bias_nb_batches[i] == 0:\n","        actual_0.append(predicted_bias[i])\n","      else:\n","        actual_1.append(predicted_bias[i])\n","    else:\n","      if bias_nb_batches[i] == 0:\n","        # actual_0[0].append(predicted_bias[i])\n","        actual_0[0] = np.append(actual_0[0], predicted_bias[i])\n","      else:\n","        # actual_1[0] = np.append(actual_1[0], predicted_bias[i])\n","        actual_1.append(predicted_bias[i])\n","  \n","  # print(type(actual_0[0]))\n","  print(actual_0[0].sort())\n","  print(actual_1)\n"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5ZqKgBSCZpu","executionInfo":{"status":"ok","timestamp":1619621530408,"user_tz":-330,"elapsed":1126,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["dataset_file = './toxic_train_30k.csv'"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Po_a7RUCgzw","executionInfo":{"status":"ok","timestamp":1619621530857,"user_tz":-330,"elapsed":869,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["nb_epochs = 1\n","# batch_size = 47\n","batch_size = 1\n","nb_batches = 1000\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","gen = batch_generator_bias(batch_size, nb_batches, dataset_file)\n","transfer_bias_model = TransferBiasClassification()\n","# transfer_bias_model = transfer_bias_model.to(device)\n","adam = optim.Adam(transfer_bias_model.parameters(), lr=learning_rate)\n","\n","train_epoch_acc = []\n","\n","PATH = \"saved_model_bias_onlyToxic1k.pt\""],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-1BndKOCg4J","executionInfo":{"status":"ok","timestamp":1619621533987,"user_tz":-330,"elapsed":1251,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["train_predictions = []"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnSb5wyRCg72","executionInfo":{"status":"ok","timestamp":1619624953525,"user_tz":-330,"elapsed":3417726,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}},"outputId":"d2cb37a2-5bbb-4a51-be32-0e0b54e3ca6b"},"source":["for epoch in range(nb_epochs):\n","\n","    train_batch_loss = []\n","    train_batch_acc = []\n","\n","    transfer_bias_nb_batches = []\n","\n","    for batch in range(nb_batches):\n","\n","        text, transfer_bias = next(gen)\n","\n","        transfer_bias_nb_batches.append(transfer_bias[0][0])\n","        \n","        out = transfer_bias_model.forward(text)\n","\n","        loss = transfer_bias_model.loss(out, transfer_bias)\n","        \n","        # out = transfer_bias_model.forward(text).to(device)\n","\n","        # loss = transfer_bias_model.loss(out, transfer_bias).to(device)\n","        print(\"Epoch:\", epoch,\n","              \"Batch:\", batch,\n","              \"Loss:\", loss.data[0])\n","\n","        adam.zero_grad()\n","        # loss.backward()\n","        loss.sum().backward()\n","        adam.step()\n","        # print(\"out\", out)\n","\n","        torch.save(transfer_bias_model.state_dict(), PATH)\n","\n","\n","        transfer_bias_model.eval() # enter evaluation mode\n","        with torch.no_grad():\n","              train_batch_acc.append(compare(out)) # evaluate mini-batch train accuracy in evaluation\n","\n","    mapPrediction(train_predictions, transfer_bias_nb_batches)\n","    acc = accuracy(train_batch_acc, transfer_bias_nb_batches)\n","    print(\"Epoch: \", epoch, \"Bias Accuracy: \", acc)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch: 0 Batch: 0 Loss: tensor(120.0497)\n","Epoch: 0 Batch: 1 Loss: tensor(119.8824)\n","Epoch: 0 Batch: 2 Loss: tensor(119.6653)\n","Epoch: 0 Batch: 3 Loss: tensor(119.4735)\n","Epoch: 0 Batch: 4 Loss: tensor(119.3098)\n","Epoch: 0 Batch: 5 Loss: tensor(119.1010)\n","Epoch: 0 Batch: 6 Loss: tensor(120.1235)\n","Epoch: 0 Batch: 7 Loss: tensor(118.7439)\n","Epoch: 0 Batch: 8 Loss: tensor(118.5689)\n","Epoch: 0 Batch: 9 Loss: tensor(118.3641)\n","Epoch: 0 Batch: 10 Loss: tensor(118.2017)\n","Epoch: 0 Batch: 11 Loss: tensor(118.0292)\n","Epoch: 0 Batch: 12 Loss: tensor(118.3523)\n","Epoch: 0 Batch: 13 Loss: tensor(117.6207)\n","Epoch: 0 Batch: 14 Loss: tensor(117.4890)\n","Epoch: 0 Batch: 15 Loss: tensor(117.3066)\n","Epoch: 0 Batch: 16 Loss: tensor(117.4569)\n","Epoch: 0 Batch: 17 Loss: tensor(116.8978)\n","Epoch: 0 Batch: 18 Loss: tensor(116.7792)\n","Epoch: 0 Batch: 19 Loss: tensor(116.5541)\n","Epoch: 0 Batch: 20 Loss: tensor(116.3680)\n","Epoch: 0 Batch: 21 Loss: tensor(116.1680)\n","Epoch: 0 Batch: 22 Loss: tensor(115.9027)\n","Epoch: 0 Batch: 23 Loss: tensor(115.7202)\n","Epoch: 0 Batch: 24 Loss: tensor(115.5445)\n","Epoch: 0 Batch: 25 Loss: tensor(115.3511)\n","Epoch: 0 Batch: 26 Loss: tensor(115.1680)\n","Epoch: 0 Batch: 27 Loss: tensor(114.9930)\n","Epoch: 0 Batch: 28 Loss: tensor(114.8540)\n","Epoch: 0 Batch: 29 Loss: tensor(114.6453)\n","Epoch: 0 Batch: 30 Loss: tensor(114.4413)\n","Epoch: 0 Batch: 31 Loss: tensor(114.2964)\n","Epoch: 0 Batch: 32 Loss: tensor(114.0762)\n","Epoch: 0 Batch: 33 Loss: tensor(113.9021)\n","Epoch: 0 Batch: 34 Loss: tensor(113.7046)\n","Epoch: 0 Batch: 35 Loss: tensor(113.5442)\n","Epoch: 0 Batch: 36 Loss: tensor(113.3365)\n","Epoch: 0 Batch: 37 Loss: tensor(113.1636)\n","Epoch: 0 Batch: 38 Loss: tensor(112.9754)\n","Epoch: 0 Batch: 39 Loss: tensor(112.7981)\n","Epoch: 0 Batch: 40 Loss: tensor(112.6361)\n","Epoch: 0 Batch: 41 Loss: tensor(112.4449)\n","Epoch: 0 Batch: 42 Loss: tensor(112.9199)\n","Epoch: 0 Batch: 43 Loss: tensor(112.7826)\n","Epoch: 0 Batch: 44 Loss: tensor(112.3871)\n","Epoch: 0 Batch: 45 Loss: tensor(111.8360)\n","Epoch: 0 Batch: 46 Loss: tensor(111.7040)\n","Epoch: 0 Batch: 47 Loss: tensor(111.6247)\n","Epoch: 0 Batch: 48 Loss: tensor(111.5071)\n","Epoch: 0 Batch: 49 Loss: tensor(111.2238)\n","Epoch: 0 Batch: 50 Loss: tensor(111.0177)\n","Epoch: 0 Batch: 51 Loss: tensor(111.0160)\n","Epoch: 0 Batch: 52 Loss: tensor(110.6274)\n","Epoch: 0 Batch: 53 Loss: tensor(110.4667)\n","Epoch: 0 Batch: 54 Loss: tensor(110.1905)\n","Epoch: 0 Batch: 55 Loss: tensor(110.5152)\n","Epoch: 0 Batch: 56 Loss: tensor(110.3038)\n","Epoch: 0 Batch: 57 Loss: tensor(109.6638)\n","Epoch: 0 Batch: 58 Loss: tensor(109.9053)\n","Epoch: 0 Batch: 59 Loss: tensor(109.7799)\n","Epoch: 0 Batch: 60 Loss: tensor(109.2728)\n","Epoch: 0 Batch: 61 Loss: tensor(109.1465)\n","Epoch: 0 Batch: 62 Loss: tensor(108.9249)\n","Epoch: 0 Batch: 63 Loss: tensor(108.8484)\n","Epoch: 0 Batch: 64 Loss: tensor(108.5355)\n","Epoch: 0 Batch: 65 Loss: tensor(108.5898)\n","Epoch: 0 Batch: 66 Loss: tensor(108.2119)\n","Epoch: 0 Batch: 67 Loss: tensor(108.0284)\n","Epoch: 0 Batch: 68 Loss: tensor(107.7644)\n","Epoch: 0 Batch: 69 Loss: tensor(107.6450)\n","Epoch: 0 Batch: 70 Loss: tensor(107.4439)\n","Epoch: 0 Batch: 71 Loss: tensor(107.2482)\n","Epoch: 0 Batch: 72 Loss: tensor(107.0619)\n","Epoch: 0 Batch: 73 Loss: tensor(106.8579)\n","Epoch: 0 Batch: 74 Loss: tensor(106.7122)\n","Epoch: 0 Batch: 75 Loss: tensor(106.5183)\n","Epoch: 0 Batch: 76 Loss: tensor(106.3633)\n","Epoch: 0 Batch: 77 Loss: tensor(106.2020)\n","Epoch: 0 Batch: 78 Loss: tensor(106.0167)\n","Epoch: 0 Batch: 79 Loss: tensor(107.2507)\n","Epoch: 0 Batch: 80 Loss: tensor(105.6949)\n","Epoch: 0 Batch: 81 Loss: tensor(105.5457)\n","Epoch: 0 Batch: 82 Loss: tensor(105.3472)\n","Epoch: 0 Batch: 83 Loss: tensor(105.1867)\n","Epoch: 0 Batch: 84 Loss: tensor(105.0183)\n","Epoch: 0 Batch: 85 Loss: tensor(104.8607)\n","Epoch: 0 Batch: 86 Loss: tensor(105.6718)\n","Epoch: 0 Batch: 87 Loss: tensor(104.5130)\n","Epoch: 0 Batch: 88 Loss: tensor(104.3892)\n","Epoch: 0 Batch: 89 Loss: tensor(104.2053)\n","Epoch: 0 Batch: 90 Loss: tensor(104.0193)\n","Epoch: 0 Batch: 91 Loss: tensor(103.8569)\n","Epoch: 0 Batch: 92 Loss: tensor(103.7058)\n","Epoch: 0 Batch: 93 Loss: tensor(103.5634)\n","Epoch: 0 Batch: 94 Loss: tensor(103.3931)\n","Epoch: 0 Batch: 95 Loss: tensor(103.2159)\n","Epoch: 0 Batch: 96 Loss: tensor(103.0861)\n","Epoch: 0 Batch: 97 Loss: tensor(102.8992)\n","Epoch: 0 Batch: 98 Loss: tensor(102.7313)\n","Epoch: 0 Batch: 99 Loss: tensor(102.5593)\n","Epoch: 0 Batch: 100 Loss: tensor(102.3869)\n","Epoch: 0 Batch: 101 Loss: tensor(102.2567)\n","Epoch: 0 Batch: 102 Loss: tensor(102.0689)\n","Epoch: 0 Batch: 103 Loss: tensor(101.9073)\n","Epoch: 0 Batch: 104 Loss: tensor(101.7478)\n","Epoch: 0 Batch: 105 Loss: tensor(102.5541)\n","Epoch: 0 Batch: 106 Loss: tensor(101.4259)\n","Epoch: 0 Batch: 107 Loss: tensor(101.2625)\n","Epoch: 0 Batch: 108 Loss: tensor(101.1066)\n","Epoch: 0 Batch: 109 Loss: tensor(100.9445)\n","Epoch: 0 Batch: 110 Loss: tensor(100.7917)\n","Epoch: 0 Batch: 111 Loss: tensor(100.6258)\n","Epoch: 0 Batch: 112 Loss: tensor(100.4679)\n","Epoch: 0 Batch: 113 Loss: tensor(100.3104)\n","Epoch: 0 Batch: 114 Loss: tensor(100.1518)\n","Epoch: 0 Batch: 115 Loss: tensor(100.0037)\n","Epoch: 0 Batch: 116 Loss: tensor(99.8364)\n","Epoch: 0 Batch: 117 Loss: tensor(99.6823)\n","Epoch: 0 Batch: 118 Loss: tensor(99.5268)\n","Epoch: 0 Batch: 119 Loss: tensor(99.3676)\n","Epoch: 0 Batch: 120 Loss: tensor(99.2107)\n","Epoch: 0 Batch: 121 Loss: tensor(99.0580)\n","Epoch: 0 Batch: 122 Loss: tensor(98.8992)\n","Epoch: 0 Batch: 123 Loss: tensor(98.7502)\n","Epoch: 0 Batch: 124 Loss: tensor(98.5997)\n","Epoch: 0 Batch: 125 Loss: tensor(98.4398)\n","Epoch: 0 Batch: 126 Loss: tensor(98.2814)\n","Epoch: 0 Batch: 127 Loss: tensor(98.1231)\n","Epoch: 0 Batch: 128 Loss: tensor(97.9687)\n","Epoch: 0 Batch: 129 Loss: tensor(97.8156)\n","Epoch: 0 Batch: 130 Loss: tensor(97.6640)\n","Epoch: 0 Batch: 131 Loss: tensor(97.5129)\n","Epoch: 0 Batch: 132 Loss: tensor(97.3542)\n","Epoch: 0 Batch: 133 Loss: tensor(97.2378)\n","Epoch: 0 Batch: 134 Loss: tensor(97.0737)\n","Epoch: 0 Batch: 135 Loss: tensor(96.9113)\n","Epoch: 0 Batch: 136 Loss: tensor(96.7455)\n","Epoch: 0 Batch: 137 Loss: tensor(96.5989)\n","Epoch: 0 Batch: 138 Loss: tensor(96.4434)\n","Epoch: 0 Batch: 139 Loss: tensor(96.2982)\n","Epoch: 0 Batch: 140 Loss: tensor(96.1414)\n","Epoch: 0 Batch: 141 Loss: tensor(95.9999)\n","Epoch: 0 Batch: 142 Loss: tensor(95.8417)\n","Epoch: 0 Batch: 143 Loss: tensor(95.6862)\n","Epoch: 0 Batch: 144 Loss: tensor(95.5369)\n","Epoch: 0 Batch: 145 Loss: tensor(95.3863)\n","Epoch: 0 Batch: 146 Loss: tensor(95.2482)\n","Epoch: 0 Batch: 147 Loss: tensor(95.0875)\n","Epoch: 0 Batch: 148 Loss: tensor(94.9385)\n","Epoch: 0 Batch: 149 Loss: tensor(94.7987)\n","Epoch: 0 Batch: 150 Loss: tensor(94.6481)\n","Epoch: 0 Batch: 151 Loss: tensor(95.4859)\n","Epoch: 0 Batch: 152 Loss: tensor(94.3472)\n","Epoch: 0 Batch: 153 Loss: tensor(94.1991)\n","Epoch: 0 Batch: 154 Loss: tensor(94.0489)\n","Epoch: 0 Batch: 155 Loss: tensor(93.9024)\n","Epoch: 0 Batch: 156 Loss: tensor(93.7624)\n","Epoch: 0 Batch: 157 Loss: tensor(93.6331)\n","Epoch: 0 Batch: 158 Loss: tensor(93.4791)\n","Epoch: 0 Batch: 159 Loss: tensor(94.0419)\n","Epoch: 0 Batch: 160 Loss: tensor(93.2144)\n","Epoch: 0 Batch: 161 Loss: tensor(93.0454)\n","Epoch: 0 Batch: 162 Loss: tensor(92.8793)\n","Epoch: 0 Batch: 163 Loss: tensor(92.8200)\n","Epoch: 0 Batch: 164 Loss: tensor(92.6475)\n","Epoch: 0 Batch: 165 Loss: tensor(92.4519)\n","Epoch: 0 Batch: 166 Loss: tensor(92.3128)\n","Epoch: 0 Batch: 167 Loss: tensor(92.1686)\n","Epoch: 0 Batch: 168 Loss: tensor(92.7407)\n","Epoch: 0 Batch: 169 Loss: tensor(91.8910)\n","Epoch: 0 Batch: 170 Loss: tensor(91.7493)\n","Epoch: 0 Batch: 171 Loss: tensor(91.5859)\n","Epoch: 0 Batch: 172 Loss: tensor(91.4629)\n","Epoch: 0 Batch: 173 Loss: tensor(91.2919)\n","Epoch: 0 Batch: 174 Loss: tensor(91.1841)\n","Epoch: 0 Batch: 175 Loss: tensor(91.0575)\n","Epoch: 0 Batch: 176 Loss: tensor(91.9175)\n","Epoch: 0 Batch: 177 Loss: tensor(90.7397)\n","Epoch: 0 Batch: 178 Loss: tensor(90.6078)\n","Epoch: 0 Batch: 179 Loss: tensor(91.1772)\n","Epoch: 0 Batch: 180 Loss: tensor(90.3013)\n","Epoch: 0 Batch: 181 Loss: tensor(90.9526)\n","Epoch: 0 Batch: 182 Loss: tensor(90.0368)\n","Epoch: 0 Batch: 183 Loss: tensor(89.9165)\n","Epoch: 0 Batch: 184 Loss: tensor(89.7776)\n","Epoch: 0 Batch: 185 Loss: tensor(89.6701)\n","Epoch: 0 Batch: 186 Loss: tensor(89.5501)\n","Epoch: 0 Batch: 187 Loss: tensor(89.3685)\n","Epoch: 0 Batch: 188 Loss: tensor(89.2237)\n","Epoch: 0 Batch: 189 Loss: tensor(89.0648)\n","Epoch: 0 Batch: 190 Loss: tensor(88.9261)\n","Epoch: 0 Batch: 191 Loss: tensor(88.8327)\n","Epoch: 0 Batch: 192 Loss: tensor(88.6820)\n","Epoch: 0 Batch: 193 Loss: tensor(88.4953)\n","Epoch: 0 Batch: 194 Loss: tensor(88.4035)\n","Epoch: 0 Batch: 195 Loss: tensor(88.2251)\n","Epoch: 0 Batch: 196 Loss: tensor(88.0682)\n","Epoch: 0 Batch: 197 Loss: tensor(87.9315)\n","Epoch: 0 Batch: 198 Loss: tensor(87.8005)\n","Epoch: 0 Batch: 199 Loss: tensor(87.6681)\n","Epoch: 0 Batch: 200 Loss: tensor(87.5210)\n","Epoch: 0 Batch: 201 Loss: tensor(88.4761)\n","Epoch: 0 Batch: 202 Loss: tensor(87.2514)\n","Epoch: 0 Batch: 203 Loss: tensor(87.1129)\n","Epoch: 0 Batch: 204 Loss: tensor(86.9757)\n","Epoch: 0 Batch: 205 Loss: tensor(86.8432)\n","Epoch: 0 Batch: 206 Loss: tensor(87.7447)\n","Epoch: 0 Batch: 207 Loss: tensor(86.5700)\n","Epoch: 0 Batch: 208 Loss: tensor(86.4456)\n","Epoch: 0 Batch: 209 Loss: tensor(86.2996)\n","Epoch: 0 Batch: 210 Loss: tensor(86.1656)\n","Epoch: 0 Batch: 211 Loss: tensor(86.8481)\n","Epoch: 0 Batch: 212 Loss: tensor(85.8983)\n","Epoch: 0 Batch: 213 Loss: tensor(85.7859)\n","Epoch: 0 Batch: 214 Loss: tensor(85.6679)\n","Epoch: 0 Batch: 215 Loss: tensor(85.5153)\n","Epoch: 0 Batch: 216 Loss: tensor(85.4073)\n","Epoch: 0 Batch: 217 Loss: tensor(85.2365)\n","Epoch: 0 Batch: 218 Loss: tensor(85.7962)\n","Epoch: 0 Batch: 219 Loss: tensor(85.0277)\n","Epoch: 0 Batch: 220 Loss: tensor(84.8516)\n","Epoch: 0 Batch: 221 Loss: tensor(84.7326)\n","Epoch: 0 Batch: 222 Loss: tensor(84.6063)\n","Epoch: 0 Batch: 223 Loss: tensor(84.5214)\n","Epoch: 0 Batch: 224 Loss: tensor(84.3821)\n","Epoch: 0 Batch: 225 Loss: tensor(84.2421)\n","Epoch: 0 Batch: 226 Loss: tensor(84.0528)\n","Epoch: 0 Batch: 227 Loss: tensor(83.9445)\n","Epoch: 0 Batch: 228 Loss: tensor(83.8287)\n","Epoch: 0 Batch: 229 Loss: tensor(83.6663)\n","Epoch: 0 Batch: 230 Loss: tensor(83.5447)\n","Epoch: 0 Batch: 231 Loss: tensor(84.1511)\n","Epoch: 0 Batch: 232 Loss: tensor(83.2762)\n","Epoch: 0 Batch: 233 Loss: tensor(83.1384)\n","Epoch: 0 Batch: 234 Loss: tensor(83.0036)\n","Epoch: 0 Batch: 235 Loss: tensor(82.8799)\n","Epoch: 0 Batch: 236 Loss: tensor(82.7468)\n","Epoch: 0 Batch: 237 Loss: tensor(82.6165)\n","Epoch: 0 Batch: 238 Loss: tensor(83.3000)\n","Epoch: 0 Batch: 239 Loss: tensor(82.3715)\n","Epoch: 0 Batch: 240 Loss: tensor(82.2365)\n","Epoch: 0 Batch: 241 Loss: tensor(82.1059)\n","Epoch: 0 Batch: 242 Loss: tensor(81.9803)\n","Epoch: 0 Batch: 243 Loss: tensor(81.8572)\n","Epoch: 0 Batch: 244 Loss: tensor(81.7274)\n","Epoch: 0 Batch: 245 Loss: tensor(81.6097)\n","Epoch: 0 Batch: 246 Loss: tensor(81.4767)\n","Epoch: 0 Batch: 247 Loss: tensor(81.3440)\n","Epoch: 0 Batch: 248 Loss: tensor(81.2211)\n","Epoch: 0 Batch: 249 Loss: tensor(81.0959)\n","Epoch: 0 Batch: 250 Loss: tensor(80.9733)\n","Epoch: 0 Batch: 251 Loss: tensor(80.8442)\n","Epoch: 0 Batch: 252 Loss: tensor(80.7217)\n","Epoch: 0 Batch: 253 Loss: tensor(80.5865)\n","Epoch: 0 Batch: 254 Loss: tensor(80.4628)\n","Epoch: 0 Batch: 255 Loss: tensor(80.3370)\n","Epoch: 0 Batch: 256 Loss: tensor(80.2117)\n","Epoch: 0 Batch: 257 Loss: tensor(80.0891)\n","Epoch: 0 Batch: 258 Loss: tensor(79.9638)\n","Epoch: 0 Batch: 259 Loss: tensor(79.8451)\n","Epoch: 0 Batch: 260 Loss: tensor(79.7163)\n","Epoch: 0 Batch: 261 Loss: tensor(79.5921)\n","Epoch: 0 Batch: 262 Loss: tensor(79.4706)\n","Epoch: 0 Batch: 263 Loss: tensor(79.3454)\n","Epoch: 0 Batch: 264 Loss: tensor(79.2227)\n","Epoch: 0 Batch: 265 Loss: tensor(79.1022)\n","Epoch: 0 Batch: 266 Loss: tensor(78.9770)\n","Epoch: 0 Batch: 267 Loss: tensor(78.8553)\n","Epoch: 0 Batch: 268 Loss: tensor(79.7303)\n","Epoch: 0 Batch: 269 Loss: tensor(78.6108)\n","Epoch: 0 Batch: 270 Loss: tensor(78.4876)\n","Epoch: 0 Batch: 271 Loss: tensor(78.3690)\n","Epoch: 0 Batch: 272 Loss: tensor(78.2469)\n","Epoch: 0 Batch: 273 Loss: tensor(78.1244)\n","Epoch: 0 Batch: 274 Loss: tensor(78.0022)\n","Epoch: 0 Batch: 275 Loss: tensor(77.8810)\n","Epoch: 0 Batch: 276 Loss: tensor(77.7627)\n","Epoch: 0 Batch: 277 Loss: tensor(77.6394)\n","Epoch: 0 Batch: 278 Loss: tensor(78.3811)\n","Epoch: 0 Batch: 279 Loss: tensor(77.4021)\n","Epoch: 0 Batch: 280 Loss: tensor(77.2867)\n","Epoch: 0 Batch: 281 Loss: tensor(77.1729)\n","Epoch: 0 Batch: 282 Loss: tensor(77.0567)\n","Epoch: 0 Batch: 283 Loss: tensor(76.9266)\n","Epoch: 0 Batch: 284 Loss: tensor(76.8077)\n","Epoch: 0 Batch: 285 Loss: tensor(76.6963)\n","Epoch: 0 Batch: 286 Loss: tensor(77.2943)\n","Epoch: 0 Batch: 287 Loss: tensor(76.4593)\n","Epoch: 0 Batch: 288 Loss: tensor(76.3472)\n","Epoch: 0 Batch: 289 Loss: tensor(76.2443)\n","Epoch: 0 Batch: 290 Loss: tensor(76.1383)\n","Epoch: 0 Batch: 291 Loss: tensor(76.0050)\n","Epoch: 0 Batch: 292 Loss: tensor(75.8950)\n","Epoch: 0 Batch: 293 Loss: tensor(75.7594)\n","Epoch: 0 Batch: 294 Loss: tensor(75.6225)\n","Epoch: 0 Batch: 295 Loss: tensor(76.2882)\n","Epoch: 0 Batch: 296 Loss: tensor(75.4088)\n","Epoch: 0 Batch: 297 Loss: tensor(75.2988)\n","Epoch: 0 Batch: 298 Loss: tensor(75.8843)\n","Epoch: 0 Batch: 299 Loss: tensor(75.0544)\n","Epoch: 0 Batch: 300 Loss: tensor(75.7483)\n","Epoch: 0 Batch: 301 Loss: tensor(74.8325)\n","Epoch: 0 Batch: 302 Loss: tensor(74.7200)\n","Epoch: 0 Batch: 303 Loss: tensor(74.6048)\n","Epoch: 0 Batch: 304 Loss: tensor(74.5225)\n","Epoch: 0 Batch: 305 Loss: tensor(74.3881)\n","Epoch: 0 Batch: 306 Loss: tensor(74.2455)\n","Epoch: 0 Batch: 307 Loss: tensor(74.1801)\n","Epoch: 0 Batch: 308 Loss: tensor(74.0377)\n","Epoch: 0 Batch: 309 Loss: tensor(73.9514)\n","Epoch: 0 Batch: 310 Loss: tensor(73.8213)\n","Epoch: 0 Batch: 311 Loss: tensor(73.6831)\n","Epoch: 0 Batch: 312 Loss: tensor(74.4044)\n","Epoch: 0 Batch: 313 Loss: tensor(73.4537)\n","Epoch: 0 Batch: 314 Loss: tensor(73.3427)\n","Epoch: 0 Batch: 315 Loss: tensor(73.2337)\n","Epoch: 0 Batch: 316 Loss: tensor(73.1221)\n","Epoch: 0 Batch: 317 Loss: tensor(72.9882)\n","Epoch: 0 Batch: 318 Loss: tensor(73.6382)\n","Epoch: 0 Batch: 319 Loss: tensor(72.7667)\n","Epoch: 0 Batch: 320 Loss: tensor(72.6498)\n","Epoch: 0 Batch: 321 Loss: tensor(72.5429)\n","Epoch: 0 Batch: 322 Loss: tensor(72.4315)\n","Epoch: 0 Batch: 323 Loss: tensor(72.3133)\n","Epoch: 0 Batch: 324 Loss: tensor(73.0065)\n","Epoch: 0 Batch: 325 Loss: tensor(72.1111)\n","Epoch: 0 Batch: 326 Loss: tensor(71.9820)\n","Epoch: 0 Batch: 327 Loss: tensor(71.8707)\n","Epoch: 0 Batch: 328 Loss: tensor(71.7681)\n","Epoch: 0 Batch: 329 Loss: tensor(71.6428)\n","Epoch: 0 Batch: 330 Loss: tensor(72.2686)\n","Epoch: 0 Batch: 331 Loss: tensor(71.4211)\n","Epoch: 0 Batch: 332 Loss: tensor(71.3079)\n","Epoch: 0 Batch: 333 Loss: tensor(71.2036)\n","Epoch: 0 Batch: 334 Loss: tensor(71.0872)\n","Epoch: 0 Batch: 335 Loss: tensor(71.7174)\n","Epoch: 0 Batch: 336 Loss: tensor(70.8880)\n","Epoch: 0 Batch: 337 Loss: tensor(70.7794)\n","Epoch: 0 Batch: 338 Loss: tensor(70.6671)\n","Epoch: 0 Batch: 339 Loss: tensor(70.5632)\n","Epoch: 0 Batch: 340 Loss: tensor(70.4666)\n","Epoch: 0 Batch: 341 Loss: tensor(70.3399)\n","Epoch: 0 Batch: 342 Loss: tensor(70.9664)\n","Epoch: 0 Batch: 343 Loss: tensor(70.1219)\n","Epoch: 0 Batch: 344 Loss: tensor(70.8088)\n","Epoch: 0 Batch: 345 Loss: tensor(69.9308)\n","Epoch: 0 Batch: 346 Loss: tensor(69.8341)\n","Epoch: 0 Batch: 347 Loss: tensor(69.7311)\n","Epoch: 0 Batch: 348 Loss: tensor(69.5838)\n","Epoch: 0 Batch: 349 Loss: tensor(69.4995)\n","Epoch: 0 Batch: 350 Loss: tensor(69.3759)\n","Epoch: 0 Batch: 351 Loss: tensor(69.2656)\n","Epoch: 0 Batch: 352 Loss: tensor(69.2003)\n","Epoch: 0 Batch: 353 Loss: tensor(69.0540)\n","Epoch: 0 Batch: 354 Loss: tensor(68.9227)\n","Epoch: 0 Batch: 355 Loss: tensor(68.8155)\n","Epoch: 0 Batch: 356 Loss: tensor(68.7100)\n","Epoch: 0 Batch: 357 Loss: tensor(68.6104)\n","Epoch: 0 Batch: 358 Loss: tensor(68.5056)\n","Epoch: 0 Batch: 359 Loss: tensor(68.3981)\n","Epoch: 0 Batch: 360 Loss: tensor(68.2792)\n","Epoch: 0 Batch: 361 Loss: tensor(68.1867)\n","Epoch: 0 Batch: 362 Loss: tensor(68.0703)\n","Epoch: 0 Batch: 363 Loss: tensor(67.9664)\n","Epoch: 0 Batch: 364 Loss: tensor(67.8606)\n","Epoch: 0 Batch: 365 Loss: tensor(67.7531)\n","Epoch: 0 Batch: 366 Loss: tensor(67.6481)\n","Epoch: 0 Batch: 367 Loss: tensor(67.5450)\n","Epoch: 0 Batch: 368 Loss: tensor(67.4397)\n","Epoch: 0 Batch: 369 Loss: tensor(67.3390)\n","Epoch: 0 Batch: 370 Loss: tensor(67.2335)\n","Epoch: 0 Batch: 371 Loss: tensor(67.1283)\n","Epoch: 0 Batch: 372 Loss: tensor(67.0242)\n","Epoch: 0 Batch: 373 Loss: tensor(66.9212)\n","Epoch: 0 Batch: 374 Loss: tensor(66.8175)\n","Epoch: 0 Batch: 375 Loss: tensor(66.7144)\n","Epoch: 0 Batch: 376 Loss: tensor(66.6114)\n","Epoch: 0 Batch: 377 Loss: tensor(66.5097)\n","Epoch: 0 Batch: 378 Loss: tensor(66.4066)\n","Epoch: 0 Batch: 379 Loss: tensor(66.3042)\n","Epoch: 0 Batch: 380 Loss: tensor(66.2040)\n","Epoch: 0 Batch: 381 Loss: tensor(66.0997)\n","Epoch: 0 Batch: 382 Loss: tensor(65.9997)\n","Epoch: 0 Batch: 383 Loss: tensor(65.8954)\n","Epoch: 0 Batch: 384 Loss: tensor(65.7936)\n","Epoch: 0 Batch: 385 Loss: tensor(65.6922)\n","Epoch: 0 Batch: 386 Loss: tensor(65.5929)\n","Epoch: 0 Batch: 387 Loss: tensor(65.4903)\n","Epoch: 0 Batch: 388 Loss: tensor(65.3894)\n","Epoch: 0 Batch: 389 Loss: tensor(65.2877)\n","Epoch: 0 Batch: 390 Loss: tensor(65.1869)\n","Epoch: 0 Batch: 391 Loss: tensor(65.0866)\n","Epoch: 0 Batch: 392 Loss: tensor(66.0577)\n","Epoch: 0 Batch: 393 Loss: tensor(64.8863)\n","Epoch: 0 Batch: 394 Loss: tensor(64.7867)\n","Epoch: 0 Batch: 395 Loss: tensor(64.6885)\n","Epoch: 0 Batch: 396 Loss: tensor(64.5866)\n","Epoch: 0 Batch: 397 Loss: tensor(64.4873)\n","Epoch: 0 Batch: 398 Loss: tensor(64.3930)\n","Epoch: 0 Batch: 399 Loss: tensor(64.2896)\n","Epoch: 0 Batch: 400 Loss: tensor(64.1918)\n","Epoch: 0 Batch: 401 Loss: tensor(64.0919)\n","Epoch: 0 Batch: 402 Loss: tensor(63.9914)\n","Epoch: 0 Batch: 403 Loss: tensor(63.9060)\n","Epoch: 0 Batch: 404 Loss: tensor(63.7986)\n","Epoch: 0 Batch: 405 Loss: tensor(63.7002)\n","Epoch: 0 Batch: 406 Loss: tensor(63.5986)\n","Epoch: 0 Batch: 407 Loss: tensor(63.4992)\n","Epoch: 0 Batch: 408 Loss: tensor(63.4028)\n","Epoch: 0 Batch: 409 Loss: tensor(63.3056)\n","Epoch: 0 Batch: 410 Loss: tensor(63.2075)\n","Epoch: 0 Batch: 411 Loss: tensor(63.1094)\n","Epoch: 0 Batch: 412 Loss: tensor(63.0256)\n","Epoch: 0 Batch: 413 Loss: tensor(62.9151)\n","Epoch: 0 Batch: 414 Loss: tensor(62.8260)\n","Epoch: 0 Batch: 415 Loss: tensor(63.6110)\n","Epoch: 0 Batch: 416 Loss: tensor(62.6298)\n","Epoch: 0 Batch: 417 Loss: tensor(62.5298)\n","Epoch: 0 Batch: 418 Loss: tensor(62.4377)\n","Epoch: 0 Batch: 419 Loss: tensor(62.3394)\n","Epoch: 0 Batch: 420 Loss: tensor(62.2471)\n","Epoch: 0 Batch: 421 Loss: tensor(62.1436)\n","Epoch: 0 Batch: 422 Loss: tensor(62.0479)\n","Epoch: 0 Batch: 423 Loss: tensor(62.8475)\n","Epoch: 0 Batch: 424 Loss: tensor(61.8626)\n","Epoch: 0 Batch: 425 Loss: tensor(61.7627)\n","Epoch: 0 Batch: 426 Loss: tensor(61.6678)\n","Epoch: 0 Batch: 427 Loss: tensor(61.5740)\n","Epoch: 0 Batch: 428 Loss: tensor(61.4819)\n","Epoch: 0 Batch: 429 Loss: tensor(62.2231)\n","Epoch: 0 Batch: 430 Loss: tensor(61.2975)\n","Epoch: 0 Batch: 431 Loss: tensor(61.1983)\n","Epoch: 0 Batch: 432 Loss: tensor(61.1075)\n","Epoch: 0 Batch: 433 Loss: tensor(61.0215)\n","Epoch: 0 Batch: 434 Loss: tensor(60.9212)\n","Epoch: 0 Batch: 435 Loss: tensor(60.8496)\n","Epoch: 0 Batch: 436 Loss: tensor(60.7351)\n","Epoch: 0 Batch: 437 Loss: tensor(61.3975)\n","Epoch: 0 Batch: 438 Loss: tensor(60.5518)\n","Epoch: 0 Batch: 439 Loss: tensor(61.1349)\n","Epoch: 0 Batch: 440 Loss: tensor(60.3778)\n","Epoch: 0 Batch: 441 Loss: tensor(60.2788)\n","Epoch: 0 Batch: 442 Loss: tensor(60.8970)\n","Epoch: 0 Batch: 443 Loss: tensor(60.1149)\n","Epoch: 0 Batch: 444 Loss: tensor(60.0302)\n","Epoch: 0 Batch: 445 Loss: tensor(59.9339)\n","Epoch: 0 Batch: 446 Loss: tensor(59.8466)\n","Epoch: 0 Batch: 447 Loss: tensor(59.7360)\n","Epoch: 0 Batch: 448 Loss: tensor(59.6459)\n","Epoch: 0 Batch: 449 Loss: tensor(59.5455)\n","Epoch: 0 Batch: 450 Loss: tensor(59.5063)\n","Epoch: 0 Batch: 451 Loss: tensor(59.9218)\n","Epoch: 0 Batch: 452 Loss: tensor(59.2845)\n","Epoch: 0 Batch: 453 Loss: tensor(59.2045)\n","Epoch: 0 Batch: 454 Loss: tensor(59.0944)\n","Epoch: 0 Batch: 455 Loss: tensor(58.9929)\n","Epoch: 0 Batch: 456 Loss: tensor(58.9293)\n","Epoch: 0 Batch: 457 Loss: tensor(58.8301)\n","Epoch: 0 Batch: 458 Loss: tensor(58.7366)\n","Epoch: 0 Batch: 459 Loss: tensor(58.6240)\n","Epoch: 0 Batch: 460 Loss: tensor(58.5420)\n","Epoch: 0 Batch: 461 Loss: tensor(58.4388)\n","Epoch: 0 Batch: 462 Loss: tensor(58.3600)\n","Epoch: 0 Batch: 463 Loss: tensor(58.2816)\n","Epoch: 0 Batch: 464 Loss: tensor(58.1783)\n","Epoch: 0 Batch: 465 Loss: tensor(58.0884)\n","Epoch: 0 Batch: 466 Loss: tensor(57.9877)\n","Epoch: 0 Batch: 467 Loss: tensor(57.8964)\n","Epoch: 0 Batch: 468 Loss: tensor(57.8077)\n","Epoch: 0 Batch: 469 Loss: tensor(57.7181)\n","Epoch: 0 Batch: 470 Loss: tensor(57.6248)\n","Epoch: 0 Batch: 471 Loss: tensor(57.5358)\n","Epoch: 0 Batch: 472 Loss: tensor(57.4496)\n","Epoch: 0 Batch: 473 Loss: tensor(57.3593)\n","Epoch: 0 Batch: 474 Loss: tensor(57.2689)\n","Epoch: 0 Batch: 475 Loss: tensor(57.1840)\n","Epoch: 0 Batch: 476 Loss: tensor(57.9760)\n","Epoch: 0 Batch: 477 Loss: tensor(57.0071)\n","Epoch: 0 Batch: 478 Loss: tensor(56.9171)\n","Epoch: 0 Batch: 479 Loss: tensor(56.8300)\n","Epoch: 0 Batch: 480 Loss: tensor(56.7421)\n","Epoch: 0 Batch: 481 Loss: tensor(56.6546)\n","Epoch: 0 Batch: 482 Loss: tensor(56.5675)\n","Epoch: 0 Batch: 483 Loss: tensor(56.4799)\n","Epoch: 0 Batch: 484 Loss: tensor(56.3959)\n","Epoch: 0 Batch: 485 Loss: tensor(56.3069)\n","Epoch: 0 Batch: 486 Loss: tensor(56.2246)\n","Epoch: 0 Batch: 487 Loss: tensor(56.1360)\n","Epoch: 0 Batch: 488 Loss: tensor(56.0470)\n","Epoch: 0 Batch: 489 Loss: tensor(55.9615)\n","Epoch: 0 Batch: 490 Loss: tensor(55.8824)\n","Epoch: 0 Batch: 491 Loss: tensor(55.7894)\n","Epoch: 0 Batch: 492 Loss: tensor(55.7052)\n","Epoch: 0 Batch: 493 Loss: tensor(55.6188)\n","Epoch: 0 Batch: 494 Loss: tensor(55.5325)\n","Epoch: 0 Batch: 495 Loss: tensor(55.4461)\n","Epoch: 0 Batch: 496 Loss: tensor(55.3610)\n","Epoch: 0 Batch: 497 Loss: tensor(56.2178)\n","Epoch: 0 Batch: 498 Loss: tensor(55.1910)\n","Epoch: 0 Batch: 499 Loss: tensor(55.1062)\n","Epoch: 0 Batch: 500 Loss: tensor(55.0224)\n","Epoch: 0 Batch: 501 Loss: tensor(54.9368)\n","Epoch: 0 Batch: 502 Loss: tensor(54.8543)\n","Epoch: 0 Batch: 503 Loss: tensor(55.6783)\n","Epoch: 0 Batch: 504 Loss: tensor(54.6835)\n","Epoch: 0 Batch: 505 Loss: tensor(54.6059)\n","Epoch: 0 Batch: 506 Loss: tensor(54.5179)\n","Epoch: 0 Batch: 507 Loss: tensor(54.4378)\n","Epoch: 0 Batch: 508 Loss: tensor(54.3534)\n","Epoch: 0 Batch: 509 Loss: tensor(54.2696)\n","Epoch: 0 Batch: 510 Loss: tensor(54.1896)\n","Epoch: 0 Batch: 511 Loss: tensor(54.1085)\n","Epoch: 0 Batch: 512 Loss: tensor(54.0211)\n","Epoch: 0 Batch: 513 Loss: tensor(53.9403)\n","Epoch: 0 Batch: 514 Loss: tensor(53.8545)\n","Epoch: 0 Batch: 515 Loss: tensor(53.7666)\n","Epoch: 0 Batch: 516 Loss: tensor(53.6946)\n","Epoch: 0 Batch: 517 Loss: tensor(53.6051)\n","Epoch: 0 Batch: 518 Loss: tensor(53.5224)\n","Epoch: 0 Batch: 519 Loss: tensor(54.3015)\n","Epoch: 0 Batch: 520 Loss: tensor(54.2182)\n","Epoch: 0 Batch: 521 Loss: tensor(54.0897)\n","Epoch: 0 Batch: 522 Loss: tensor(53.1924)\n","Epoch: 0 Batch: 523 Loss: tensor(53.1141)\n","Epoch: 0 Batch: 524 Loss: tensor(53.0467)\n","Epoch: 0 Batch: 525 Loss: tensor(52.9695)\n","Epoch: 0 Batch: 526 Loss: tensor(52.8847)\n","Epoch: 0 Batch: 527 Loss: tensor(53.4414)\n","Epoch: 0 Batch: 528 Loss: tensor(52.7226)\n","Epoch: 0 Batch: 529 Loss: tensor(53.3330)\n","Epoch: 0 Batch: 530 Loss: tensor(52.5739)\n","Epoch: 0 Batch: 531 Loss: tensor(52.5234)\n","Epoch: 0 Batch: 532 Loss: tensor(52.4242)\n","Epoch: 0 Batch: 533 Loss: tensor(52.3590)\n","Epoch: 0 Batch: 534 Loss: tensor(52.2681)\n","Epoch: 0 Batch: 535 Loss: tensor(52.2149)\n","Epoch: 0 Batch: 536 Loss: tensor(52.1286)\n","Epoch: 0 Batch: 537 Loss: tensor(52.0385)\n","Epoch: 0 Batch: 538 Loss: tensor(51.9264)\n","Epoch: 0 Batch: 539 Loss: tensor(51.8623)\n","Epoch: 0 Batch: 540 Loss: tensor(51.7942)\n","Epoch: 0 Batch: 541 Loss: tensor(51.6959)\n","Epoch: 0 Batch: 542 Loss: tensor(51.6005)\n","Epoch: 0 Batch: 543 Loss: tensor(51.5359)\n","Epoch: 0 Batch: 544 Loss: tensor(51.4742)\n","Epoch: 0 Batch: 545 Loss: tensor(51.3569)\n","Epoch: 0 Batch: 546 Loss: tensor(51.2736)\n","Epoch: 0 Batch: 547 Loss: tensor(51.2204)\n","Epoch: 0 Batch: 548 Loss: tensor(51.1099)\n","Epoch: 0 Batch: 549 Loss: tensor(51.0265)\n","Epoch: 0 Batch: 550 Loss: tensor(50.9548)\n","Epoch: 0 Batch: 551 Loss: tensor(50.8800)\n","Epoch: 0 Batch: 552 Loss: tensor(50.7930)\n","Epoch: 0 Batch: 553 Loss: tensor(50.7157)\n","Epoch: 0 Batch: 554 Loss: tensor(50.6399)\n","Epoch: 0 Batch: 555 Loss: tensor(50.5541)\n","Epoch: 0 Batch: 556 Loss: tensor(50.4780)\n","Epoch: 0 Batch: 557 Loss: tensor(50.3996)\n","Epoch: 0 Batch: 558 Loss: tensor(50.3210)\n","Epoch: 0 Batch: 559 Loss: tensor(50.2436)\n","Epoch: 0 Batch: 560 Loss: tensor(50.1666)\n","Epoch: 0 Batch: 561 Loss: tensor(50.0886)\n","Epoch: 0 Batch: 562 Loss: tensor(50.0115)\n","Epoch: 0 Batch: 563 Loss: tensor(49.9362)\n","Epoch: 0 Batch: 564 Loss: tensor(49.8576)\n","Epoch: 0 Batch: 565 Loss: tensor(49.7815)\n","Epoch: 0 Batch: 566 Loss: tensor(49.7045)\n","Epoch: 0 Batch: 567 Loss: tensor(49.6330)\n","Epoch: 0 Batch: 568 Loss: tensor(49.5521)\n","Epoch: 0 Batch: 569 Loss: tensor(49.4762)\n","Epoch: 0 Batch: 570 Loss: tensor(49.4008)\n","Epoch: 0 Batch: 571 Loss: tensor(49.3235)\n","Epoch: 0 Batch: 572 Loss: tensor(49.2477)\n","Epoch: 0 Batch: 573 Loss: tensor(49.1727)\n","Epoch: 0 Batch: 574 Loss: tensor(49.0979)\n","Epoch: 0 Batch: 575 Loss: tensor(49.0221)\n","Epoch: 0 Batch: 576 Loss: tensor(48.9449)\n","Epoch: 0 Batch: 577 Loss: tensor(48.8699)\n","Epoch: 0 Batch: 578 Loss: tensor(48.7960)\n","Epoch: 0 Batch: 579 Loss: tensor(49.7798)\n","Epoch: 0 Batch: 580 Loss: tensor(48.6473)\n","Epoch: 0 Batch: 581 Loss: tensor(48.5704)\n","Epoch: 0 Batch: 582 Loss: tensor(49.5161)\n","Epoch: 0 Batch: 583 Loss: tensor(48.4214)\n","Epoch: 0 Batch: 584 Loss: tensor(48.3475)\n","Epoch: 0 Batch: 585 Loss: tensor(48.2737)\n","Epoch: 0 Batch: 586 Loss: tensor(48.1993)\n","Epoch: 0 Batch: 587 Loss: tensor(48.1236)\n","Epoch: 0 Batch: 588 Loss: tensor(48.0496)\n","Epoch: 0 Batch: 589 Loss: tensor(47.9762)\n","Epoch: 0 Batch: 590 Loss: tensor(48.8556)\n","Epoch: 0 Batch: 591 Loss: tensor(47.8291)\n","Epoch: 0 Batch: 592 Loss: tensor(47.7626)\n","Epoch: 0 Batch: 593 Loss: tensor(47.6839)\n","Epoch: 0 Batch: 594 Loss: tensor(47.6102)\n","Epoch: 0 Batch: 595 Loss: tensor(47.5402)\n","Epoch: 0 Batch: 596 Loss: tensor(47.4646)\n","Epoch: 0 Batch: 597 Loss: tensor(47.3976)\n","Epoch: 0 Batch: 598 Loss: tensor(47.3219)\n","Epoch: 0 Batch: 599 Loss: tensor(47.2467)\n","Epoch: 0 Batch: 600 Loss: tensor(47.9829)\n","Epoch: 0 Batch: 601 Loss: tensor(47.1096)\n","Epoch: 0 Batch: 602 Loss: tensor(47.8451)\n","Epoch: 0 Batch: 603 Loss: tensor(46.9599)\n","Epoch: 0 Batch: 604 Loss: tensor(47.7071)\n","Epoch: 0 Batch: 605 Loss: tensor(46.8229)\n","Epoch: 0 Batch: 606 Loss: tensor(46.7549)\n","Epoch: 0 Batch: 607 Loss: tensor(46.6876)\n","Epoch: 0 Batch: 608 Loss: tensor(47.3682)\n","Epoch: 0 Batch: 609 Loss: tensor(46.5440)\n","Epoch: 0 Batch: 610 Loss: tensor(47.0976)\n","Epoch: 0 Batch: 611 Loss: tensor(46.4241)\n","Epoch: 0 Batch: 612 Loss: tensor(46.3658)\n","Epoch: 0 Batch: 613 Loss: tensor(46.2654)\n","Epoch: 0 Batch: 614 Loss: tensor(46.1987)\n","Epoch: 0 Batch: 615 Loss: tensor(46.1332)\n","Epoch: 0 Batch: 616 Loss: tensor(46.0667)\n","Epoch: 0 Batch: 617 Loss: tensor(45.9993)\n","Epoch: 0 Batch: 618 Loss: tensor(45.9226)\n","Epoch: 0 Batch: 619 Loss: tensor(45.8532)\n","Epoch: 0 Batch: 620 Loss: tensor(45.7934)\n","Epoch: 0 Batch: 621 Loss: tensor(45.7238)\n","Epoch: 0 Batch: 622 Loss: tensor(45.6372)\n","Epoch: 0 Batch: 623 Loss: tensor(45.5596)\n","Epoch: 0 Batch: 624 Loss: tensor(45.5030)\n","Epoch: 0 Batch: 625 Loss: tensor(45.4154)\n","Epoch: 0 Batch: 626 Loss: tensor(45.3552)\n","Epoch: 0 Batch: 627 Loss: tensor(45.2820)\n","Epoch: 0 Batch: 628 Loss: tensor(45.2064)\n","Epoch: 0 Batch: 629 Loss: tensor(45.1403)\n","Epoch: 0 Batch: 630 Loss: tensor(45.0595)\n","Epoch: 0 Batch: 631 Loss: tensor(44.9913)\n","Epoch: 0 Batch: 632 Loss: tensor(45.7250)\n","Epoch: 0 Batch: 633 Loss: tensor(44.8519)\n","Epoch: 0 Batch: 634 Loss: tensor(44.7700)\n","Epoch: 0 Batch: 635 Loss: tensor(44.7124)\n","Epoch: 0 Batch: 636 Loss: tensor(44.6298)\n","Epoch: 0 Batch: 637 Loss: tensor(44.5648)\n","Epoch: 0 Batch: 638 Loss: tensor(45.2641)\n","Epoch: 0 Batch: 639 Loss: tensor(44.4288)\n","Epoch: 0 Batch: 640 Loss: tensor(44.3613)\n","Epoch: 0 Batch: 641 Loss: tensor(45.0677)\n","Epoch: 0 Batch: 642 Loss: tensor(45.0256)\n","Epoch: 0 Batch: 643 Loss: tensor(45.0035)\n","Epoch: 0 Batch: 644 Loss: tensor(44.0958)\n","Epoch: 0 Batch: 645 Loss: tensor(44.0276)\n","Epoch: 0 Batch: 646 Loss: tensor(43.9714)\n","Epoch: 0 Batch: 647 Loss: tensor(43.8875)\n","Epoch: 0 Batch: 648 Loss: tensor(43.8300)\n","Epoch: 0 Batch: 649 Loss: tensor(43.7699)\n","Epoch: 0 Batch: 650 Loss: tensor(43.7037)\n","Epoch: 0 Batch: 651 Loss: tensor(43.6240)\n","Epoch: 0 Batch: 652 Loss: tensor(43.5832)\n","Epoch: 0 Batch: 653 Loss: tensor(43.4970)\n","Epoch: 0 Batch: 654 Loss: tensor(43.4405)\n","Epoch: 0 Batch: 655 Loss: tensor(44.0762)\n","Epoch: 0 Batch: 656 Loss: tensor(43.3126)\n","Epoch: 0 Batch: 657 Loss: tensor(43.2368)\n","Epoch: 0 Batch: 658 Loss: tensor(43.1657)\n","Epoch: 0 Batch: 659 Loss: tensor(43.1058)\n","Epoch: 0 Batch: 660 Loss: tensor(43.0489)\n","Epoch: 0 Batch: 661 Loss: tensor(42.9661)\n","Epoch: 0 Batch: 662 Loss: tensor(43.6212)\n","Epoch: 0 Batch: 663 Loss: tensor(42.8485)\n","Epoch: 0 Batch: 664 Loss: tensor(42.7570)\n","Epoch: 0 Batch: 665 Loss: tensor(43.3551)\n","Epoch: 0 Batch: 666 Loss: tensor(42.6510)\n","Epoch: 0 Batch: 667 Loss: tensor(42.5686)\n","Epoch: 0 Batch: 668 Loss: tensor(42.5051)\n","Epoch: 0 Batch: 669 Loss: tensor(42.4433)\n","Epoch: 0 Batch: 670 Loss: tensor(42.3830)\n","Epoch: 0 Batch: 671 Loss: tensor(42.3044)\n","Epoch: 0 Batch: 672 Loss: tensor(42.2445)\n","Epoch: 0 Batch: 673 Loss: tensor(42.1800)\n","Epoch: 0 Batch: 674 Loss: tensor(42.1090)\n","Epoch: 0 Batch: 675 Loss: tensor(42.0465)\n","Epoch: 0 Batch: 676 Loss: tensor(41.9944)\n","Epoch: 0 Batch: 677 Loss: tensor(41.9052)\n","Epoch: 0 Batch: 678 Loss: tensor(41.8487)\n","Epoch: 0 Batch: 679 Loss: tensor(42.5334)\n","Epoch: 0 Batch: 680 Loss: tensor(41.7196)\n","Epoch: 0 Batch: 681 Loss: tensor(41.6534)\n","Epoch: 0 Batch: 682 Loss: tensor(41.6048)\n","Epoch: 0 Batch: 683 Loss: tensor(41.5192)\n","Epoch: 0 Batch: 684 Loss: tensor(41.4662)\n","Epoch: 0 Batch: 685 Loss: tensor(41.3998)\n","Epoch: 0 Batch: 686 Loss: tensor(41.3268)\n","Epoch: 0 Batch: 687 Loss: tensor(42.0555)\n","Epoch: 0 Batch: 688 Loss: tensor(41.1990)\n","Epoch: 0 Batch: 689 Loss: tensor(41.1533)\n","Epoch: 0 Batch: 690 Loss: tensor(41.0905)\n","Epoch: 0 Batch: 691 Loss: tensor(41.0158)\n","Epoch: 0 Batch: 692 Loss: tensor(40.9442)\n","Epoch: 0 Batch: 693 Loss: tensor(40.8894)\n","Epoch: 0 Batch: 694 Loss: tensor(40.8250)\n","Epoch: 0 Batch: 695 Loss: tensor(40.7617)\n","Epoch: 0 Batch: 696 Loss: tensor(40.7004)\n","Epoch: 0 Batch: 697 Loss: tensor(40.6318)\n","Epoch: 0 Batch: 698 Loss: tensor(41.3756)\n","Epoch: 0 Batch: 699 Loss: tensor(40.5059)\n","Epoch: 0 Batch: 700 Loss: tensor(41.2799)\n","Epoch: 0 Batch: 701 Loss: tensor(40.3922)\n","Epoch: 0 Batch: 702 Loss: tensor(40.3238)\n","Epoch: 0 Batch: 703 Loss: tensor(40.2663)\n","Epoch: 0 Batch: 704 Loss: tensor(40.1974)\n","Epoch: 0 Batch: 705 Loss: tensor(40.1415)\n","Epoch: 0 Batch: 706 Loss: tensor(40.0736)\n","Epoch: 0 Batch: 707 Loss: tensor(40.0144)\n","Epoch: 0 Batch: 708 Loss: tensor(39.9546)\n","Epoch: 0 Batch: 709 Loss: tensor(39.8928)\n","Epoch: 0 Batch: 710 Loss: tensor(39.8300)\n","Epoch: 0 Batch: 711 Loss: tensor(39.7712)\n","Epoch: 0 Batch: 712 Loss: tensor(39.7060)\n","Epoch: 0 Batch: 713 Loss: tensor(39.6409)\n","Epoch: 0 Batch: 714 Loss: tensor(39.5763)\n","Epoch: 0 Batch: 715 Loss: tensor(39.5191)\n","Epoch: 0 Batch: 716 Loss: tensor(39.4579)\n","Epoch: 0 Batch: 717 Loss: tensor(39.4007)\n","Epoch: 0 Batch: 718 Loss: tensor(40.1790)\n","Epoch: 0 Batch: 719 Loss: tensor(39.2796)\n","Epoch: 0 Batch: 720 Loss: tensor(39.2158)\n","Epoch: 0 Batch: 721 Loss: tensor(39.1604)\n","Epoch: 0 Batch: 722 Loss: tensor(39.8763)\n","Epoch: 0 Batch: 723 Loss: tensor(39.0327)\n","Epoch: 0 Batch: 724 Loss: tensor(38.9745)\n","Epoch: 0 Batch: 725 Loss: tensor(38.9148)\n","Epoch: 0 Batch: 726 Loss: tensor(39.6358)\n","Epoch: 0 Batch: 727 Loss: tensor(38.8038)\n","Epoch: 0 Batch: 728 Loss: tensor(38.7510)\n","Epoch: 0 Batch: 729 Loss: tensor(38.6768)\n","Epoch: 0 Batch: 730 Loss: tensor(39.3650)\n","Epoch: 0 Batch: 731 Loss: tensor(38.5617)\n","Epoch: 0 Batch: 732 Loss: tensor(39.2466)\n","Epoch: 0 Batch: 733 Loss: tensor(38.4584)\n","Epoch: 0 Batch: 734 Loss: tensor(38.3867)\n","Epoch: 0 Batch: 735 Loss: tensor(38.3348)\n","Epoch: 0 Batch: 736 Loss: tensor(38.2720)\n","Epoch: 0 Batch: 737 Loss: tensor(38.9343)\n","Epoch: 0 Batch: 738 Loss: tensor(38.1551)\n","Epoch: 0 Batch: 739 Loss: tensor(38.1052)\n","Epoch: 0 Batch: 740 Loss: tensor(38.0416)\n","Epoch: 0 Batch: 741 Loss: tensor(37.9906)\n","Epoch: 0 Batch: 742 Loss: tensor(37.9356)\n","Epoch: 0 Batch: 743 Loss: tensor(37.8836)\n","Epoch: 0 Batch: 744 Loss: tensor(37.8138)\n","Epoch: 0 Batch: 745 Loss: tensor(37.7773)\n","Epoch: 0 Batch: 746 Loss: tensor(38.3995)\n","Epoch: 0 Batch: 747 Loss: tensor(37.6360)\n","Epoch: 0 Batch: 748 Loss: tensor(37.6010)\n","Epoch: 0 Batch: 749 Loss: tensor(37.5378)\n","Epoch: 0 Batch: 750 Loss: tensor(37.4586)\n","Epoch: 0 Batch: 751 Loss: tensor(37.4010)\n","Epoch: 0 Batch: 752 Loss: tensor(37.3581)\n","Epoch: 0 Batch: 753 Loss: tensor(37.2912)\n","Epoch: 0 Batch: 754 Loss: tensor(37.2344)\n","Epoch: 0 Batch: 755 Loss: tensor(37.1915)\n","Epoch: 0 Batch: 756 Loss: tensor(37.7569)\n","Epoch: 0 Batch: 757 Loss: tensor(37.0516)\n","Epoch: 0 Batch: 758 Loss: tensor(37.0009)\n","Epoch: 0 Batch: 759 Loss: tensor(36.9329)\n","Epoch: 0 Batch: 760 Loss: tensor(36.8834)\n","Epoch: 0 Batch: 761 Loss: tensor(37.4973)\n","Epoch: 0 Batch: 762 Loss: tensor(36.7711)\n","Epoch: 0 Batch: 763 Loss: tensor(36.7129)\n","Epoch: 0 Batch: 764 Loss: tensor(36.6555)\n","Epoch: 0 Batch: 765 Loss: tensor(36.5948)\n","Epoch: 0 Batch: 766 Loss: tensor(36.5459)\n","Epoch: 0 Batch: 767 Loss: tensor(36.4914)\n","Epoch: 0 Batch: 768 Loss: tensor(36.4278)\n","Epoch: 0 Batch: 769 Loss: tensor(36.3734)\n","Epoch: 0 Batch: 770 Loss: tensor(36.3153)\n","Epoch: 0 Batch: 771 Loss: tensor(36.2557)\n","Epoch: 0 Batch: 772 Loss: tensor(36.2014)\n","Epoch: 0 Batch: 773 Loss: tensor(36.1437)\n","Epoch: 0 Batch: 774 Loss: tensor(36.0831)\n","Epoch: 0 Batch: 775 Loss: tensor(36.0257)\n","Epoch: 0 Batch: 776 Loss: tensor(35.9725)\n","Epoch: 0 Batch: 777 Loss: tensor(35.9160)\n","Epoch: 0 Batch: 778 Loss: tensor(35.8619)\n","Epoch: 0 Batch: 779 Loss: tensor(35.8037)\n","Epoch: 0 Batch: 780 Loss: tensor(35.7493)\n","Epoch: 0 Batch: 781 Loss: tensor(35.6903)\n","Epoch: 0 Batch: 782 Loss: tensor(35.6387)\n","Epoch: 0 Batch: 783 Loss: tensor(35.5900)\n","Epoch: 0 Batch: 784 Loss: tensor(35.5253)\n","Epoch: 0 Batch: 785 Loss: tensor(35.4705)\n","Epoch: 0 Batch: 786 Loss: tensor(35.4156)\n","Epoch: 0 Batch: 787 Loss: tensor(35.3608)\n","Epoch: 0 Batch: 788 Loss: tensor(35.3055)\n","Epoch: 0 Batch: 789 Loss: tensor(35.2499)\n","Epoch: 0 Batch: 790 Loss: tensor(35.1975)\n","Epoch: 0 Batch: 791 Loss: tensor(35.1412)\n","Epoch: 0 Batch: 792 Loss: tensor(35.0856)\n","Epoch: 0 Batch: 793 Loss: tensor(35.0331)\n","Epoch: 0 Batch: 794 Loss: tensor(34.9818)\n","Epoch: 0 Batch: 795 Loss: tensor(34.9231)\n","Epoch: 0 Batch: 796 Loss: tensor(34.8702)\n","Epoch: 0 Batch: 797 Loss: tensor(34.8178)\n","Epoch: 0 Batch: 798 Loss: tensor(34.7623)\n","Epoch: 0 Batch: 799 Loss: tensor(34.7085)\n","Epoch: 0 Batch: 800 Loss: tensor(34.6545)\n","Epoch: 0 Batch: 801 Loss: tensor(34.6007)\n","Epoch: 0 Batch: 802 Loss: tensor(35.4857)\n","Epoch: 0 Batch: 803 Loss: tensor(34.4950)\n","Epoch: 0 Batch: 804 Loss: tensor(34.4423)\n","Epoch: 0 Batch: 805 Loss: tensor(34.3872)\n","Epoch: 0 Batch: 806 Loss: tensor(35.2598)\n","Epoch: 0 Batch: 807 Loss: tensor(35.2041)\n","Epoch: 0 Batch: 808 Loss: tensor(34.2314)\n","Epoch: 0 Batch: 809 Loss: tensor(34.1753)\n","Epoch: 0 Batch: 810 Loss: tensor(34.1324)\n","Epoch: 0 Batch: 811 Loss: tensor(34.0719)\n","Epoch: 0 Batch: 812 Loss: tensor(34.0260)\n","Epoch: 0 Batch: 813 Loss: tensor(33.9677)\n","Epoch: 0 Batch: 814 Loss: tensor(33.9255)\n","Epoch: 0 Batch: 815 Loss: tensor(34.7530)\n","Epoch: 0 Batch: 816 Loss: tensor(33.8117)\n","Epoch: 0 Batch: 817 Loss: tensor(34.6256)\n","Epoch: 0 Batch: 818 Loss: tensor(33.7120)\n","Epoch: 0 Batch: 819 Loss: tensor(33.6643)\n","Epoch: 0 Batch: 820 Loss: tensor(33.6056)\n","Epoch: 0 Batch: 821 Loss: tensor(33.5542)\n","Epoch: 0 Batch: 822 Loss: tensor(33.5028)\n","Epoch: 0 Batch: 823 Loss: tensor(33.4585)\n","Epoch: 0 Batch: 824 Loss: tensor(33.4106)\n","Epoch: 0 Batch: 825 Loss: tensor(33.3474)\n","Epoch: 0 Batch: 826 Loss: tensor(33.2977)\n","Epoch: 0 Batch: 827 Loss: tensor(33.9947)\n","Epoch: 0 Batch: 828 Loss: tensor(33.2062)\n","Epoch: 0 Batch: 829 Loss: tensor(33.1508)\n","Epoch: 0 Batch: 830 Loss: tensor(33.8309)\n","Epoch: 0 Batch: 831 Loss: tensor(33.0522)\n","Epoch: 0 Batch: 832 Loss: tensor(33.0042)\n","Epoch: 0 Batch: 833 Loss: tensor(32.9558)\n","Epoch: 0 Batch: 834 Loss: tensor(32.9012)\n","Epoch: 0 Batch: 835 Loss: tensor(32.8492)\n","Epoch: 0 Batch: 836 Loss: tensor(32.8103)\n","Epoch: 0 Batch: 837 Loss: tensor(32.7483)\n","Epoch: 0 Batch: 838 Loss: tensor(32.6978)\n","Epoch: 0 Batch: 839 Loss: tensor(32.6522)\n","Epoch: 0 Batch: 840 Loss: tensor(32.6008)\n","Epoch: 0 Batch: 841 Loss: tensor(32.5514)\n","Epoch: 0 Batch: 842 Loss: tensor(32.4996)\n","Epoch: 0 Batch: 843 Loss: tensor(32.4414)\n","Epoch: 0 Batch: 844 Loss: tensor(32.3905)\n","Epoch: 0 Batch: 845 Loss: tensor(32.3349)\n","Epoch: 0 Batch: 846 Loss: tensor(32.2898)\n","Epoch: 0 Batch: 847 Loss: tensor(32.2539)\n","Epoch: 0 Batch: 848 Loss: tensor(32.1936)\n","Epoch: 0 Batch: 849 Loss: tensor(32.1353)\n","Epoch: 0 Batch: 850 Loss: tensor(32.0827)\n","Epoch: 0 Batch: 851 Loss: tensor(32.0342)\n","Epoch: 0 Batch: 852 Loss: tensor(32.7435)\n","Epoch: 0 Batch: 853 Loss: tensor(32.7620)\n","Epoch: 0 Batch: 854 Loss: tensor(31.8846)\n","Epoch: 0 Batch: 855 Loss: tensor(31.8355)\n","Epoch: 0 Batch: 856 Loss: tensor(31.7924)\n","Epoch: 0 Batch: 857 Loss: tensor(31.7418)\n","Epoch: 0 Batch: 858 Loss: tensor(31.6997)\n","Epoch: 0 Batch: 859 Loss: tensor(31.6543)\n","Epoch: 0 Batch: 860 Loss: tensor(31.5929)\n","Epoch: 0 Batch: 861 Loss: tensor(31.5402)\n","Epoch: 0 Batch: 862 Loss: tensor(31.4976)\n","Epoch: 0 Batch: 863 Loss: tensor(31.4470)\n","Epoch: 0 Batch: 864 Loss: tensor(31.3938)\n","Epoch: 0 Batch: 865 Loss: tensor(31.3453)\n","Epoch: 0 Batch: 866 Loss: tensor(31.3031)\n","Epoch: 0 Batch: 867 Loss: tensor(31.2513)\n","Epoch: 0 Batch: 868 Loss: tensor(31.2067)\n","Epoch: 0 Batch: 869 Loss: tensor(31.1540)\n","Epoch: 0 Batch: 870 Loss: tensor(31.1149)\n","Epoch: 0 Batch: 871 Loss: tensor(31.0537)\n","Epoch: 0 Batch: 872 Loss: tensor(31.0063)\n","Epoch: 0 Batch: 873 Loss: tensor(30.9552)\n","Epoch: 0 Batch: 874 Loss: tensor(30.9107)\n","Epoch: 0 Batch: 875 Loss: tensor(30.8653)\n","Epoch: 0 Batch: 876 Loss: tensor(30.8166)\n","Epoch: 0 Batch: 877 Loss: tensor(31.6446)\n","Epoch: 0 Batch: 878 Loss: tensor(30.7210)\n","Epoch: 0 Batch: 879 Loss: tensor(30.6710)\n","Epoch: 0 Batch: 880 Loss: tensor(30.6244)\n","Epoch: 0 Batch: 881 Loss: tensor(30.5742)\n","Epoch: 0 Batch: 882 Loss: tensor(30.5294)\n","Epoch: 0 Batch: 883 Loss: tensor(30.4840)\n","Epoch: 0 Batch: 884 Loss: tensor(31.2950)\n","Epoch: 0 Batch: 885 Loss: tensor(30.3878)\n","Epoch: 0 Batch: 886 Loss: tensor(30.3457)\n","Epoch: 0 Batch: 887 Loss: tensor(31.0921)\n","Epoch: 0 Batch: 888 Loss: tensor(31.0616)\n","Epoch: 0 Batch: 889 Loss: tensor(30.2068)\n","Epoch: 0 Batch: 890 Loss: tensor(30.1561)\n","Epoch: 0 Batch: 891 Loss: tensor(30.1078)\n","Epoch: 0 Batch: 892 Loss: tensor(30.0670)\n","Epoch: 0 Batch: 893 Loss: tensor(30.0164)\n","Epoch: 0 Batch: 894 Loss: tensor(29.9736)\n","Epoch: 0 Batch: 895 Loss: tensor(29.9233)\n","Epoch: 0 Batch: 896 Loss: tensor(29.8801)\n","Epoch: 0 Batch: 897 Loss: tensor(29.8344)\n","Epoch: 0 Batch: 898 Loss: tensor(29.7888)\n","Epoch: 0 Batch: 899 Loss: tensor(29.7478)\n","Epoch: 0 Batch: 900 Loss: tensor(29.6962)\n","Epoch: 0 Batch: 901 Loss: tensor(29.6619)\n","Epoch: 0 Batch: 902 Loss: tensor(29.6038)\n","Epoch: 0 Batch: 903 Loss: tensor(29.5557)\n","Epoch: 0 Batch: 904 Loss: tensor(29.5092)\n","Epoch: 0 Batch: 905 Loss: tensor(29.4662)\n","Epoch: 0 Batch: 906 Loss: tensor(29.4177)\n","Epoch: 0 Batch: 907 Loss: tensor(29.3781)\n","Epoch: 0 Batch: 908 Loss: tensor(29.3263)\n","Epoch: 0 Batch: 909 Loss: tensor(29.2818)\n","Epoch: 0 Batch: 910 Loss: tensor(29.2321)\n","Epoch: 0 Batch: 911 Loss: tensor(29.1915)\n","Epoch: 0 Batch: 912 Loss: tensor(29.1463)\n","Epoch: 0 Batch: 913 Loss: tensor(29.1027)\n","Epoch: 0 Batch: 914 Loss: tensor(29.0522)\n","Epoch: 0 Batch: 915 Loss: tensor(29.0140)\n","Epoch: 0 Batch: 916 Loss: tensor(28.9674)\n","Epoch: 0 Batch: 917 Loss: tensor(28.9204)\n","Epoch: 0 Batch: 918 Loss: tensor(28.8730)\n","Epoch: 0 Batch: 919 Loss: tensor(28.8315)\n","Epoch: 0 Batch: 920 Loss: tensor(28.7796)\n","Epoch: 0 Batch: 921 Loss: tensor(28.7371)\n","Epoch: 0 Batch: 922 Loss: tensor(28.6904)\n","Epoch: 0 Batch: 923 Loss: tensor(28.6466)\n","Epoch: 0 Batch: 924 Loss: tensor(28.6031)\n","Epoch: 0 Batch: 925 Loss: tensor(29.4806)\n","Epoch: 0 Batch: 926 Loss: tensor(28.5147)\n","Epoch: 0 Batch: 927 Loss: tensor(28.4698)\n","Epoch: 0 Batch: 928 Loss: tensor(28.4234)\n","Epoch: 0 Batch: 929 Loss: tensor(28.3797)\n","Epoch: 0 Batch: 930 Loss: tensor(28.3375)\n","Epoch: 0 Batch: 931 Loss: tensor(29.2229)\n","Epoch: 0 Batch: 932 Loss: tensor(28.2492)\n","Epoch: 0 Batch: 933 Loss: tensor(28.2053)\n","Epoch: 0 Batch: 934 Loss: tensor(29.0356)\n","Epoch: 0 Batch: 935 Loss: tensor(28.1206)\n","Epoch: 0 Batch: 936 Loss: tensor(28.0748)\n","Epoch: 0 Batch: 937 Loss: tensor(28.0319)\n","Epoch: 0 Batch: 938 Loss: tensor(27.9957)\n","Epoch: 0 Batch: 939 Loss: tensor(27.9487)\n","Epoch: 0 Batch: 940 Loss: tensor(27.9074)\n","Epoch: 0 Batch: 941 Loss: tensor(27.8607)\n","Epoch: 0 Batch: 942 Loss: tensor(27.8202)\n","Epoch: 0 Batch: 943 Loss: tensor(27.7751)\n","Epoch: 0 Batch: 944 Loss: tensor(27.7313)\n","Epoch: 0 Batch: 945 Loss: tensor(27.6904)\n","Epoch: 0 Batch: 946 Loss: tensor(27.6482)\n","Epoch: 0 Batch: 947 Loss: tensor(27.6036)\n","Epoch: 0 Batch: 948 Loss: tensor(27.5598)\n","Epoch: 0 Batch: 949 Loss: tensor(27.5175)\n","Epoch: 0 Batch: 950 Loss: tensor(27.4791)\n","Epoch: 0 Batch: 951 Loss: tensor(27.4310)\n","Epoch: 0 Batch: 952 Loss: tensor(28.2858)\n","Epoch: 0 Batch: 953 Loss: tensor(27.3480)\n","Epoch: 0 Batch: 954 Loss: tensor(27.3062)\n","Epoch: 0 Batch: 955 Loss: tensor(27.2644)\n","Epoch: 0 Batch: 956 Loss: tensor(27.2175)\n","Epoch: 0 Batch: 957 Loss: tensor(27.9707)\n","Epoch: 0 Batch: 958 Loss: tensor(27.9489)\n","Epoch: 0 Batch: 959 Loss: tensor(27.0944)\n","Epoch: 0 Batch: 960 Loss: tensor(27.0530)\n","Epoch: 0 Batch: 961 Loss: tensor(27.0128)\n","Epoch: 0 Batch: 962 Loss: tensor(26.9726)\n","Epoch: 0 Batch: 963 Loss: tensor(26.9247)\n","Epoch: 0 Batch: 964 Loss: tensor(26.8884)\n","Epoch: 0 Batch: 965 Loss: tensor(27.6125)\n","Epoch: 0 Batch: 966 Loss: tensor(26.8107)\n","Epoch: 0 Batch: 967 Loss: tensor(26.7698)\n","Epoch: 0 Batch: 968 Loss: tensor(26.7288)\n","Epoch: 0 Batch: 969 Loss: tensor(26.6831)\n","Epoch: 0 Batch: 970 Loss: tensor(26.6436)\n","Epoch: 0 Batch: 971 Loss: tensor(27.2963)\n","Epoch: 0 Batch: 972 Loss: tensor(26.5592)\n","Epoch: 0 Batch: 973 Loss: tensor(26.5246)\n","Epoch: 0 Batch: 974 Loss: tensor(27.2181)\n","Epoch: 0 Batch: 975 Loss: tensor(26.4453)\n","Epoch: 0 Batch: 976 Loss: tensor(26.3976)\n","Epoch: 0 Batch: 977 Loss: tensor(26.3623)\n","Epoch: 0 Batch: 978 Loss: tensor(26.3235)\n","Epoch: 0 Batch: 979 Loss: tensor(26.2771)\n","Epoch: 0 Batch: 980 Loss: tensor(26.2437)\n","Epoch: 0 Batch: 981 Loss: tensor(26.2021)\n","Epoch: 0 Batch: 982 Loss: tensor(26.8502)\n","Epoch: 0 Batch: 983 Loss: tensor(26.1206)\n","Epoch: 0 Batch: 984 Loss: tensor(26.0807)\n","Epoch: 0 Batch: 985 Loss: tensor(26.0377)\n","Epoch: 0 Batch: 986 Loss: tensor(26.0027)\n","Epoch: 0 Batch: 987 Loss: tensor(25.9673)\n","Epoch: 0 Batch: 988 Loss: tensor(25.9217)\n","Epoch: 0 Batch: 989 Loss: tensor(25.8756)\n","Epoch: 0 Batch: 990 Loss: tensor(25.8363)\n","Epoch: 0 Batch: 991 Loss: tensor(25.8034)\n","Epoch: 0 Batch: 992 Loss: tensor(25.7595)\n","Epoch: 0 Batch: 993 Loss: tensor(25.7233)\n","Epoch: 0 Batch: 994 Loss: tensor(25.6782)\n","Epoch: 0 Batch: 995 Loss: tensor(25.6369)\n","Epoch: 0 Batch: 996 Loss: tensor(25.5902)\n","Epoch: 0 Batch: 997 Loss: tensor(25.5496)\n","Epoch: 0 Batch: 998 Loss: tensor(25.5056)\n","Epoch: 0 Batch: 999 Loss: tensor(25.4781)\n","None\n","[array(-0.10469123, dtype=float32), array(0.23579626, dtype=float32), array(0.33695078, dtype=float32), array(0.185455, dtype=float32), array(0.1608952, dtype=float32), array(0.30199608, dtype=float32), array(0.4010337, dtype=float32), array(0.2522781, dtype=float32), array(0.2767598, dtype=float32), array(0.31166956, dtype=float32), array(0.2785215, dtype=float32), array(0.39279675, dtype=float32), array(-0.18590558, dtype=float32), array(0.0033747, dtype=float32), array(0.01414312, dtype=float32), array(0.00324009, dtype=float32), array(0.14689106, dtype=float32), array(0.14420313, dtype=float32), array(-0.02608851, dtype=float32), array(0.14104332, dtype=float32), array(0.10741964, dtype=float32), array(-0.04520166, dtype=float32), array(-0.02015034, dtype=float32), array(0.09562898, dtype=float32), array(0.164493, dtype=float32), array(0.12896973, dtype=float32), array(0.09911796, dtype=float32), array(0.00057023, dtype=float32), array(0.07136555, dtype=float32), array(0.14474519, dtype=float32), array(0.11361164, dtype=float32), array(0.14453442, dtype=float32), array(0.08941343, dtype=float32), array(0.07035381, dtype=float32), array(0.1179687, dtype=float32), array(0.0946568, dtype=float32), array(0.13478866, dtype=float32), array(0.13437304, dtype=float32), array(0.12714857, dtype=float32), array(0.09417352, dtype=float32), array(-0.03495394, dtype=float32), array(0.05647346, dtype=float32), array(0.0536353, dtype=float32), array(0.0828302, dtype=float32), array(0.12319942, dtype=float32), array(0.16753991, dtype=float32), array(0.14339413, dtype=float32), array(0.23381373, dtype=float32), array(0.06037702, dtype=float32), array(0.02958217, dtype=float32), array(0.04550534, dtype=float32), array(0.06892079, dtype=float32), array(0.06948112, dtype=float32), array(0.09482504, dtype=float32), array(0.18718003, dtype=float32), array(0.1550374, dtype=float32), array(-0.02955393, dtype=float32), array(-0.01035204, dtype=float32), array(0.02334293, dtype=float32), array(0.09832095, dtype=float32), array(0.09452832, dtype=float32), array(0.09095252, dtype=float32), array(0.1191494, dtype=float32), array(0.19463243, dtype=float32), array(0.09303167, dtype=float32), array(0.11970066, dtype=float32), array(0.11461597, dtype=float32), array(0.10020311, dtype=float32), array(0.07499339, dtype=float32), array(0.14161095, dtype=float32), array(0.13586362, dtype=float32), array(0.17640984, dtype=float32), array(0.12412722, dtype=float32), array(0.10500826, dtype=float32), array(0.09737962, dtype=float32), array(0.08137661, dtype=float32), array(0.07831974, dtype=float32), array(0.11193681, dtype=float32), array(0.11212739, dtype=float32), array(0.13009328, dtype=float32), array(0.12988684, dtype=float32), array(0.1394321, dtype=float32), array(0.14413963, dtype=float32), array(0.18345821, dtype=float32), array(0.16818896, dtype=float32), array(0.03126966, dtype=float32), array(0.03784041, dtype=float32), array(0.03930691, dtype=float32), array(0.05496673, dtype=float32), array(0.06722228, dtype=float32), array(0.12989385, dtype=float32), array(0.13561119, dtype=float32), array(0.12422282, dtype=float32), array(0.0862276, dtype=float32), array(0.06069797, dtype=float32), array(0.07013901, dtype=float32), array(0.10404982, dtype=float32), array(0.09478307, dtype=float32), array(0.03835322, dtype=float32), array(0.0346164, dtype=float32), array(0.06399409, dtype=float32), array(0.04990145, dtype=float32), array(0.10583254, dtype=float32), array(0.09439449, dtype=float32), array(0.11844309, dtype=float32), array(0.15732425, dtype=float32), array(0.13074778, dtype=float32), array(0.15433781, dtype=float32)]\n","[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Epoch:  0 Bias Accuracy:  0.614\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VE8gUmFTNtVL"},"source":["Olid Dataset"]},{"cell_type":"code","metadata":{"id":"iWGkVSOoChDQ","executionInfo":{"status":"ok","timestamp":1619624953528,"user_tz":-330,"elapsed":3344641,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["dataset_file = './olid-train.csv'"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUDGR9hSChHJ","executionInfo":{"status":"ok","timestamp":1619624953529,"user_tz":-330,"elapsed":3343754,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}}},"source":["nb_epochs = 1\n","# batch_size = 47\n","batch_size = 1\n","nb_batches = 1000\n","# 13240\n","\n","gen = batch_generator_bias(batch_size, nb_batches, dataset_file)\n","# transfer_bias_model = TransferBiasClassification()\n","# adam = optim.Adam(transfer_bias_model.parameters(), lr=learning_rate)\n","\n","train_epoch_acc = []\n","PATH = \"saved_model_bias_toxic30kAndOlid1k.pt\""],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzKvW_wEChKV","executionInfo":{"status":"ok","timestamp":1619628374070,"user_tz":-330,"elapsed":6761417,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}},"outputId":"7d00d19a-31d6-4ca0-c3d0-20ae60ea74e3"},"source":["for epoch in range(nb_epochs):\n","\n","    train_batch_loss = []\n","    train_batch_acc = []\n","\n","    transfer_bias_nb_batches = []\n","\n","    for batch in range(nb_batches):\n","\n","        text, transfer_bias = next(gen)\n","\n","        transfer_bias_nb_batches.append(transfer_bias[0][0])\n","        \n","        out = transfer_bias_model.forward(text)\n","\n","        loss = transfer_bias_model.loss(out, transfer_bias)\n","        print(\"Epoch:\", epoch,\n","              \"Batch:\", batch,\n","              \"Loss:\", loss.data[0])\n","\n","        adam.zero_grad()\n","        # loss.backward()\n","        loss.sum().backward()\n","        adam.step()\n","        # print(\"out\", out)\n","\n","        torch.save(transfer_bias_model.state_dict(), PATH)\n","\n","\n","        transfer_bias_model.eval() # enter evaluation mode\n","        with torch.no_grad():\n","              train_batch_acc.append(compare(out)) # evaluate mini-batch train accuracy in evaluation\n","\n","    acc = accuracy(train_batch_acc, transfer_bias_nb_batches)\n","    print(\"Epoch: \", epoch, \"Bias Accuracy: \", acc)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Epoch: 0 Batch: 0 Loss: tensor(25.4229)\n","Epoch: 0 Batch: 1 Loss: tensor(25.3836)\n","Epoch: 0 Batch: 2 Loss: tensor(25.3448)\n","Epoch: 0 Batch: 3 Loss: tensor(25.3022)\n","Epoch: 0 Batch: 4 Loss: tensor(25.2597)\n","Epoch: 0 Batch: 5 Loss: tensor(25.2249)\n","Epoch: 0 Batch: 6 Loss: tensor(25.1780)\n","Epoch: 0 Batch: 7 Loss: tensor(25.1428)\n","Epoch: 0 Batch: 8 Loss: tensor(25.1072)\n","Epoch: 0 Batch: 9 Loss: tensor(25.0696)\n","Epoch: 0 Batch: 10 Loss: tensor(25.0233)\n","Epoch: 0 Batch: 11 Loss: tensor(24.9831)\n","Epoch: 0 Batch: 12 Loss: tensor(24.9458)\n","Epoch: 0 Batch: 13 Loss: tensor(24.9053)\n","Epoch: 0 Batch: 14 Loss: tensor(24.8660)\n","Epoch: 0 Batch: 15 Loss: tensor(24.8304)\n","Epoch: 0 Batch: 16 Loss: tensor(24.7898)\n","Epoch: 0 Batch: 17 Loss: tensor(24.7496)\n","Epoch: 0 Batch: 18 Loss: tensor(24.7137)\n","Epoch: 0 Batch: 19 Loss: tensor(24.6735)\n","Epoch: 0 Batch: 20 Loss: tensor(24.6334)\n","Epoch: 0 Batch: 21 Loss: tensor(24.5968)\n","Epoch: 0 Batch: 22 Loss: tensor(24.5571)\n","Epoch: 0 Batch: 23 Loss: tensor(24.5183)\n","Epoch: 0 Batch: 24 Loss: tensor(24.4807)\n","Epoch: 0 Batch: 25 Loss: tensor(24.4426)\n","Epoch: 0 Batch: 26 Loss: tensor(24.4041)\n","Epoch: 0 Batch: 27 Loss: tensor(24.3680)\n","Epoch: 0 Batch: 28 Loss: tensor(24.3280)\n","Epoch: 0 Batch: 29 Loss: tensor(24.2896)\n","Epoch: 0 Batch: 30 Loss: tensor(24.2524)\n","Epoch: 0 Batch: 31 Loss: tensor(24.2145)\n","Epoch: 0 Batch: 32 Loss: tensor(24.1765)\n","Epoch: 0 Batch: 33 Loss: tensor(24.1386)\n","Epoch: 0 Batch: 34 Loss: tensor(24.1010)\n","Epoch: 0 Batch: 35 Loss: tensor(24.0647)\n","Epoch: 0 Batch: 36 Loss: tensor(24.0270)\n","Epoch: 0 Batch: 37 Loss: tensor(23.9885)\n","Epoch: 0 Batch: 38 Loss: tensor(23.9508)\n","Epoch: 0 Batch: 39 Loss: tensor(23.9138)\n","Epoch: 0 Batch: 40 Loss: tensor(23.8780)\n","Epoch: 0 Batch: 41 Loss: tensor(23.8402)\n","Epoch: 0 Batch: 42 Loss: tensor(23.8020)\n","Epoch: 0 Batch: 43 Loss: tensor(23.7651)\n","Epoch: 0 Batch: 44 Loss: tensor(23.7279)\n","Epoch: 0 Batch: 45 Loss: tensor(23.6909)\n","Epoch: 0 Batch: 46 Loss: tensor(23.6540)\n","Epoch: 0 Batch: 47 Loss: tensor(23.6176)\n","Epoch: 0 Batch: 48 Loss: tensor(23.5802)\n","Epoch: 0 Batch: 49 Loss: tensor(23.5435)\n","Epoch: 0 Batch: 50 Loss: tensor(23.5074)\n","Epoch: 0 Batch: 51 Loss: tensor(23.4699)\n","Epoch: 0 Batch: 52 Loss: tensor(23.4335)\n","Epoch: 0 Batch: 53 Loss: tensor(23.3971)\n","Epoch: 0 Batch: 54 Loss: tensor(23.3604)\n","Epoch: 0 Batch: 55 Loss: tensor(23.3239)\n","Epoch: 0 Batch: 56 Loss: tensor(23.2874)\n","Epoch: 0 Batch: 57 Loss: tensor(23.2510)\n","Epoch: 0 Batch: 58 Loss: tensor(23.2151)\n","Epoch: 0 Batch: 59 Loss: tensor(23.1787)\n","Epoch: 0 Batch: 60 Loss: tensor(23.1424)\n","Epoch: 0 Batch: 61 Loss: tensor(23.1066)\n","Epoch: 0 Batch: 62 Loss: tensor(23.0704)\n","Epoch: 0 Batch: 63 Loss: tensor(23.0342)\n","Epoch: 0 Batch: 64 Loss: tensor(22.9982)\n","Epoch: 0 Batch: 65 Loss: tensor(22.9622)\n","Epoch: 0 Batch: 66 Loss: tensor(22.9265)\n","Epoch: 0 Batch: 67 Loss: tensor(22.8907)\n","Epoch: 0 Batch: 68 Loss: tensor(22.8548)\n","Epoch: 0 Batch: 69 Loss: tensor(22.8191)\n","Epoch: 0 Batch: 70 Loss: tensor(22.7840)\n","Epoch: 0 Batch: 71 Loss: tensor(22.7485)\n","Epoch: 0 Batch: 72 Loss: tensor(22.7124)\n","Epoch: 0 Batch: 73 Loss: tensor(22.6780)\n","Epoch: 0 Batch: 74 Loss: tensor(22.6416)\n","Epoch: 0 Batch: 75 Loss: tensor(22.6061)\n","Epoch: 0 Batch: 76 Loss: tensor(22.5708)\n","Epoch: 0 Batch: 77 Loss: tensor(22.5356)\n","Epoch: 0 Batch: 78 Loss: tensor(22.5010)\n","Epoch: 0 Batch: 79 Loss: tensor(22.4650)\n","Epoch: 0 Batch: 80 Loss: tensor(22.4320)\n","Epoch: 0 Batch: 81 Loss: tensor(22.3955)\n","Epoch: 0 Batch: 82 Loss: tensor(22.3602)\n","Epoch: 0 Batch: 83 Loss: tensor(22.3260)\n","Epoch: 0 Batch: 84 Loss: tensor(22.2910)\n","Epoch: 0 Batch: 85 Loss: tensor(22.2555)\n","Epoch: 0 Batch: 86 Loss: tensor(22.2206)\n","Epoch: 0 Batch: 87 Loss: tensor(22.1859)\n","Epoch: 0 Batch: 88 Loss: tensor(22.1511)\n","Epoch: 0 Batch: 89 Loss: tensor(22.1165)\n","Epoch: 0 Batch: 90 Loss: tensor(22.0830)\n","Epoch: 0 Batch: 91 Loss: tensor(22.0478)\n","Epoch: 0 Batch: 92 Loss: tensor(22.0129)\n","Epoch: 0 Batch: 93 Loss: tensor(21.9791)\n","Epoch: 0 Batch: 94 Loss: tensor(21.9442)\n","Epoch: 0 Batch: 95 Loss: tensor(21.9103)\n","Epoch: 0 Batch: 96 Loss: tensor(21.8753)\n","Epoch: 0 Batch: 97 Loss: tensor(21.8412)\n","Epoch: 0 Batch: 98 Loss: tensor(21.8072)\n","Epoch: 0 Batch: 99 Loss: tensor(21.7729)\n","Epoch: 0 Batch: 100 Loss: tensor(21.7395)\n","Epoch: 0 Batch: 101 Loss: tensor(21.7060)\n","Epoch: 0 Batch: 102 Loss: tensor(21.6711)\n","Epoch: 0 Batch: 103 Loss: tensor(21.6374)\n","Epoch: 0 Batch: 104 Loss: tensor(21.6034)\n","Epoch: 0 Batch: 105 Loss: tensor(21.5695)\n","Epoch: 0 Batch: 106 Loss: tensor(21.5358)\n","Epoch: 0 Batch: 107 Loss: tensor(21.5021)\n","Epoch: 0 Batch: 108 Loss: tensor(21.4685)\n","Epoch: 0 Batch: 109 Loss: tensor(21.4348)\n","Epoch: 0 Batch: 110 Loss: tensor(21.4014)\n","Epoch: 0 Batch: 111 Loss: tensor(21.3682)\n","Epoch: 0 Batch: 112 Loss: tensor(21.3343)\n","Epoch: 0 Batch: 113 Loss: tensor(21.3012)\n","Epoch: 0 Batch: 114 Loss: tensor(21.2674)\n","Epoch: 0 Batch: 115 Loss: tensor(21.2341)\n","Epoch: 0 Batch: 116 Loss: tensor(21.2012)\n","Epoch: 0 Batch: 117 Loss: tensor(21.1678)\n","Epoch: 0 Batch: 118 Loss: tensor(21.1346)\n","Epoch: 0 Batch: 119 Loss: tensor(21.1017)\n","Epoch: 0 Batch: 120 Loss: tensor(21.0693)\n","Epoch: 0 Batch: 121 Loss: tensor(21.0355)\n","Epoch: 0 Batch: 122 Loss: tensor(21.0024)\n","Epoch: 0 Batch: 123 Loss: tensor(20.9698)\n","Epoch: 0 Batch: 124 Loss: tensor(20.9368)\n","Epoch: 0 Batch: 125 Loss: tensor(20.9041)\n","Epoch: 0 Batch: 126 Loss: tensor(20.8714)\n","Epoch: 0 Batch: 127 Loss: tensor(20.8396)\n","Epoch: 0 Batch: 128 Loss: tensor(20.8059)\n","Epoch: 0 Batch: 129 Loss: tensor(20.7737)\n","Epoch: 0 Batch: 130 Loss: tensor(20.7409)\n","Epoch: 0 Batch: 131 Loss: tensor(20.7083)\n","Epoch: 0 Batch: 132 Loss: tensor(20.6760)\n","Epoch: 0 Batch: 133 Loss: tensor(20.6441)\n","Epoch: 0 Batch: 134 Loss: tensor(20.6111)\n","Epoch: 0 Batch: 135 Loss: tensor(20.5794)\n","Epoch: 0 Batch: 136 Loss: tensor(20.5464)\n","Epoch: 0 Batch: 137 Loss: tensor(20.5143)\n","Epoch: 0 Batch: 138 Loss: tensor(20.4821)\n","Epoch: 0 Batch: 139 Loss: tensor(20.4499)\n","Epoch: 0 Batch: 140 Loss: tensor(20.4181)\n","Epoch: 0 Batch: 141 Loss: tensor(20.3860)\n","Epoch: 0 Batch: 142 Loss: tensor(20.3538)\n","Epoch: 0 Batch: 143 Loss: tensor(20.3238)\n","Epoch: 0 Batch: 144 Loss: tensor(20.2908)\n","Epoch: 0 Batch: 145 Loss: tensor(20.2584)\n","Epoch: 0 Batch: 146 Loss: tensor(20.2271)\n","Epoch: 0 Batch: 147 Loss: tensor(20.1950)\n","Epoch: 0 Batch: 148 Loss: tensor(20.1635)\n","Epoch: 0 Batch: 149 Loss: tensor(20.1316)\n","Epoch: 0 Batch: 150 Loss: tensor(20.0999)\n","Epoch: 0 Batch: 151 Loss: tensor(20.0683)\n","Epoch: 0 Batch: 152 Loss: tensor(20.0373)\n","Epoch: 0 Batch: 153 Loss: tensor(20.0056)\n","Epoch: 0 Batch: 154 Loss: tensor(19.9742)\n","Epoch: 0 Batch: 155 Loss: tensor(19.9427)\n","Epoch: 0 Batch: 156 Loss: tensor(19.9115)\n","Epoch: 0 Batch: 157 Loss: tensor(19.8801)\n","Epoch: 0 Batch: 158 Loss: tensor(19.8489)\n","Epoch: 0 Batch: 159 Loss: tensor(19.8177)\n","Epoch: 0 Batch: 160 Loss: tensor(19.7868)\n","Epoch: 0 Batch: 161 Loss: tensor(19.7555)\n","Epoch: 0 Batch: 162 Loss: tensor(19.7250)\n","Epoch: 0 Batch: 163 Loss: tensor(19.6936)\n","Epoch: 0 Batch: 164 Loss: tensor(19.6628)\n","Epoch: 0 Batch: 165 Loss: tensor(19.6321)\n","Epoch: 0 Batch: 166 Loss: tensor(19.6011)\n","Epoch: 0 Batch: 167 Loss: tensor(19.5702)\n","Epoch: 0 Batch: 168 Loss: tensor(19.5398)\n","Epoch: 0 Batch: 169 Loss: tensor(19.5087)\n","Epoch: 0 Batch: 170 Loss: tensor(19.4781)\n","Epoch: 0 Batch: 171 Loss: tensor(19.4474)\n","Epoch: 0 Batch: 172 Loss: tensor(19.4169)\n","Epoch: 0 Batch: 173 Loss: tensor(19.3864)\n","Epoch: 0 Batch: 174 Loss: tensor(19.3563)\n","Epoch: 0 Batch: 175 Loss: tensor(19.3257)\n","Epoch: 0 Batch: 176 Loss: tensor(19.2954)\n","Epoch: 0 Batch: 177 Loss: tensor(19.2650)\n","Epoch: 0 Batch: 178 Loss: tensor(19.2346)\n","Epoch: 0 Batch: 179 Loss: tensor(19.2051)\n","Epoch: 0 Batch: 180 Loss: tensor(19.1745)\n","Epoch: 0 Batch: 181 Loss: tensor(19.1440)\n","Epoch: 0 Batch: 182 Loss: tensor(19.1140)\n","Epoch: 0 Batch: 183 Loss: tensor(19.0839)\n","Epoch: 0 Batch: 184 Loss: tensor(19.0539)\n","Epoch: 0 Batch: 185 Loss: tensor(19.0240)\n","Epoch: 0 Batch: 186 Loss: tensor(18.9940)\n","Epoch: 0 Batch: 187 Loss: tensor(18.9641)\n","Epoch: 0 Batch: 188 Loss: tensor(18.9345)\n","Epoch: 0 Batch: 189 Loss: tensor(18.9047)\n","Epoch: 0 Batch: 190 Loss: tensor(18.8748)\n","Epoch: 0 Batch: 191 Loss: tensor(18.8451)\n","Epoch: 0 Batch: 192 Loss: tensor(18.8155)\n","Epoch: 0 Batch: 193 Loss: tensor(18.7861)\n","Epoch: 0 Batch: 194 Loss: tensor(18.7565)\n","Epoch: 0 Batch: 195 Loss: tensor(18.7271)\n","Epoch: 0 Batch: 196 Loss: tensor(18.6973)\n","Epoch: 0 Batch: 197 Loss: tensor(18.6679)\n","Epoch: 0 Batch: 198 Loss: tensor(18.6385)\n","Epoch: 0 Batch: 199 Loss: tensor(18.6096)\n","Epoch: 0 Batch: 200 Loss: tensor(18.5801)\n","Epoch: 0 Batch: 201 Loss: tensor(18.5507)\n","Epoch: 0 Batch: 202 Loss: tensor(18.5215)\n","Epoch: 0 Batch: 203 Loss: tensor(18.4924)\n","Epoch: 0 Batch: 204 Loss: tensor(18.4632)\n","Epoch: 0 Batch: 205 Loss: tensor(18.4341)\n","Epoch: 0 Batch: 206 Loss: tensor(18.4053)\n","Epoch: 0 Batch: 207 Loss: tensor(18.3761)\n","Epoch: 0 Batch: 208 Loss: tensor(18.3475)\n","Epoch: 0 Batch: 209 Loss: tensor(18.3183)\n","Epoch: 0 Batch: 210 Loss: tensor(18.2895)\n","Epoch: 0 Batch: 211 Loss: tensor(18.2606)\n","Epoch: 0 Batch: 212 Loss: tensor(18.2319)\n","Epoch: 0 Batch: 213 Loss: tensor(18.2041)\n","Epoch: 0 Batch: 214 Loss: tensor(18.1748)\n","Epoch: 0 Batch: 215 Loss: tensor(18.1459)\n","Epoch: 0 Batch: 216 Loss: tensor(18.1175)\n","Epoch: 0 Batch: 217 Loss: tensor(18.0897)\n","Epoch: 0 Batch: 218 Loss: tensor(18.0603)\n","Epoch: 0 Batch: 219 Loss: tensor(18.0320)\n","Epoch: 0 Batch: 220 Loss: tensor(18.0039)\n","Epoch: 0 Batch: 221 Loss: tensor(17.9752)\n","Epoch: 0 Batch: 222 Loss: tensor(17.9467)\n","Epoch: 0 Batch: 223 Loss: tensor(17.9185)\n","Epoch: 0 Batch: 224 Loss: tensor(17.8905)\n","Epoch: 0 Batch: 225 Loss: tensor(17.8622)\n","Epoch: 0 Batch: 226 Loss: tensor(17.8342)\n","Epoch: 0 Batch: 227 Loss: tensor(17.8057)\n","Epoch: 0 Batch: 228 Loss: tensor(17.7778)\n","Epoch: 0 Batch: 229 Loss: tensor(17.7498)\n","Epoch: 0 Batch: 230 Loss: tensor(17.7216)\n","Epoch: 0 Batch: 231 Loss: tensor(17.6938)\n","Epoch: 0 Batch: 232 Loss: tensor(17.6661)\n","Epoch: 0 Batch: 233 Loss: tensor(17.6381)\n","Epoch: 0 Batch: 234 Loss: tensor(17.6101)\n","Epoch: 0 Batch: 235 Loss: tensor(17.5827)\n","Epoch: 0 Batch: 236 Loss: tensor(17.5546)\n","Epoch: 0 Batch: 237 Loss: tensor(17.5271)\n","Epoch: 0 Batch: 238 Loss: tensor(17.4993)\n","Epoch: 0 Batch: 239 Loss: tensor(17.4717)\n","Epoch: 0 Batch: 240 Loss: tensor(17.4440)\n","Epoch: 0 Batch: 241 Loss: tensor(17.4165)\n","Epoch: 0 Batch: 242 Loss: tensor(17.3891)\n","Epoch: 0 Batch: 243 Loss: tensor(17.3617)\n","Epoch: 0 Batch: 244 Loss: tensor(17.3345)\n","Epoch: 0 Batch: 245 Loss: tensor(17.3069)\n","Epoch: 0 Batch: 246 Loss: tensor(17.2795)\n","Epoch: 0 Batch: 247 Loss: tensor(17.2523)\n","Epoch: 0 Batch: 248 Loss: tensor(17.2252)\n","Epoch: 0 Batch: 249 Loss: tensor(17.1979)\n","Epoch: 0 Batch: 250 Loss: tensor(17.1709)\n","Epoch: 0 Batch: 251 Loss: tensor(17.1436)\n","Epoch: 0 Batch: 252 Loss: tensor(17.1165)\n","Epoch: 0 Batch: 253 Loss: tensor(17.0897)\n","Epoch: 0 Batch: 254 Loss: tensor(17.0627)\n","Epoch: 0 Batch: 255 Loss: tensor(17.0361)\n","Epoch: 0 Batch: 256 Loss: tensor(17.0086)\n","Epoch: 0 Batch: 257 Loss: tensor(16.9820)\n","Epoch: 0 Batch: 258 Loss: tensor(16.9551)\n","Epoch: 0 Batch: 259 Loss: tensor(16.9286)\n","Epoch: 0 Batch: 260 Loss: tensor(16.9016)\n","Epoch: 0 Batch: 261 Loss: tensor(16.8748)\n","Epoch: 0 Batch: 262 Loss: tensor(16.8483)\n","Epoch: 0 Batch: 263 Loss: tensor(16.8214)\n","Epoch: 0 Batch: 264 Loss: tensor(16.7949)\n","Epoch: 0 Batch: 265 Loss: tensor(16.7685)\n","Epoch: 0 Batch: 266 Loss: tensor(16.7420)\n","Epoch: 0 Batch: 267 Loss: tensor(16.7155)\n","Epoch: 0 Batch: 268 Loss: tensor(16.6890)\n","Epoch: 0 Batch: 269 Loss: tensor(16.6627)\n","Epoch: 0 Batch: 270 Loss: tensor(16.6364)\n","Epoch: 0 Batch: 271 Loss: tensor(16.6099)\n","Epoch: 0 Batch: 272 Loss: tensor(16.5837)\n","Epoch: 0 Batch: 273 Loss: tensor(16.5577)\n","Epoch: 0 Batch: 274 Loss: tensor(16.5315)\n","Epoch: 0 Batch: 275 Loss: tensor(16.5051)\n","Epoch: 0 Batch: 276 Loss: tensor(16.4792)\n","Epoch: 0 Batch: 277 Loss: tensor(16.4530)\n","Epoch: 0 Batch: 278 Loss: tensor(16.4270)\n","Epoch: 0 Batch: 279 Loss: tensor(16.4009)\n","Epoch: 0 Batch: 280 Loss: tensor(16.3750)\n","Epoch: 0 Batch: 281 Loss: tensor(16.3490)\n","Epoch: 0 Batch: 282 Loss: tensor(16.3232)\n","Epoch: 0 Batch: 283 Loss: tensor(16.2973)\n","Epoch: 0 Batch: 284 Loss: tensor(16.2717)\n","Epoch: 0 Batch: 285 Loss: tensor(16.2460)\n","Epoch: 0 Batch: 286 Loss: tensor(16.2201)\n","Epoch: 0 Batch: 287 Loss: tensor(16.1944)\n","Epoch: 0 Batch: 288 Loss: tensor(16.1690)\n","Epoch: 0 Batch: 289 Loss: tensor(16.1432)\n","Epoch: 0 Batch: 290 Loss: tensor(16.1178)\n","Epoch: 0 Batch: 291 Loss: tensor(16.0922)\n","Epoch: 0 Batch: 292 Loss: tensor(16.0673)\n","Epoch: 0 Batch: 293 Loss: tensor(16.0411)\n","Epoch: 0 Batch: 294 Loss: tensor(16.0162)\n","Epoch: 0 Batch: 295 Loss: tensor(15.9904)\n","Epoch: 0 Batch: 296 Loss: tensor(15.9650)\n","Epoch: 0 Batch: 297 Loss: tensor(15.9399)\n","Epoch: 0 Batch: 298 Loss: tensor(15.9146)\n","Epoch: 0 Batch: 299 Loss: tensor(15.8896)\n","Epoch: 0 Batch: 300 Loss: tensor(15.8641)\n","Epoch: 0 Batch: 301 Loss: tensor(15.8389)\n","Epoch: 0 Batch: 302 Loss: tensor(15.8139)\n","Epoch: 0 Batch: 303 Loss: tensor(15.7888)\n","Epoch: 0 Batch: 304 Loss: tensor(15.7641)\n","Epoch: 0 Batch: 305 Loss: tensor(15.7387)\n","Epoch: 0 Batch: 306 Loss: tensor(15.7140)\n","Epoch: 0 Batch: 307 Loss: tensor(15.6892)\n","Epoch: 0 Batch: 308 Loss: tensor(15.6642)\n","Epoch: 0 Batch: 309 Loss: tensor(15.6392)\n","Epoch: 0 Batch: 310 Loss: tensor(15.6145)\n","Epoch: 0 Batch: 311 Loss: tensor(15.5897)\n","Epoch: 0 Batch: 312 Loss: tensor(15.5651)\n","Epoch: 0 Batch: 313 Loss: tensor(15.5403)\n","Epoch: 0 Batch: 314 Loss: tensor(15.5157)\n","Epoch: 0 Batch: 315 Loss: tensor(15.4911)\n","Epoch: 0 Batch: 316 Loss: tensor(15.4665)\n","Epoch: 0 Batch: 317 Loss: tensor(15.4420)\n","Epoch: 0 Batch: 318 Loss: tensor(15.4180)\n","Epoch: 0 Batch: 319 Loss: tensor(15.3930)\n","Epoch: 0 Batch: 320 Loss: tensor(15.3686)\n","Epoch: 0 Batch: 321 Loss: tensor(15.3442)\n","Epoch: 0 Batch: 322 Loss: tensor(15.3203)\n","Epoch: 0 Batch: 323 Loss: tensor(15.2955)\n","Epoch: 0 Batch: 324 Loss: tensor(15.2712)\n","Epoch: 0 Batch: 325 Loss: tensor(15.2475)\n","Epoch: 0 Batch: 326 Loss: tensor(15.2228)\n","Epoch: 0 Batch: 327 Loss: tensor(15.1989)\n","Epoch: 0 Batch: 328 Loss: tensor(15.1744)\n","Epoch: 0 Batch: 329 Loss: tensor(15.1504)\n","Epoch: 0 Batch: 330 Loss: tensor(15.1264)\n","Epoch: 0 Batch: 331 Loss: tensor(15.1024)\n","Epoch: 0 Batch: 332 Loss: tensor(15.0784)\n","Epoch: 0 Batch: 333 Loss: tensor(15.0544)\n","Epoch: 0 Batch: 334 Loss: tensor(15.0307)\n","Epoch: 0 Batch: 335 Loss: tensor(15.0067)\n","Epoch: 0 Batch: 336 Loss: tensor(14.9833)\n","Epoch: 0 Batch: 337 Loss: tensor(14.9592)\n","Epoch: 0 Batch: 338 Loss: tensor(14.9352)\n","Epoch: 0 Batch: 339 Loss: tensor(14.9115)\n","Epoch: 0 Batch: 340 Loss: tensor(14.8880)\n","Epoch: 0 Batch: 341 Loss: tensor(14.8641)\n","Epoch: 0 Batch: 342 Loss: tensor(14.8406)\n","Epoch: 0 Batch: 343 Loss: tensor(14.8170)\n","Epoch: 0 Batch: 344 Loss: tensor(14.7934)\n","Epoch: 0 Batch: 345 Loss: tensor(14.7698)\n","Epoch: 0 Batch: 346 Loss: tensor(14.7463)\n","Epoch: 0 Batch: 347 Loss: tensor(14.7230)\n","Epoch: 0 Batch: 348 Loss: tensor(14.6995)\n","Epoch: 0 Batch: 349 Loss: tensor(14.6762)\n","Epoch: 0 Batch: 350 Loss: tensor(14.6529)\n","Epoch: 0 Batch: 351 Loss: tensor(14.6295)\n","Epoch: 0 Batch: 352 Loss: tensor(14.6062)\n","Epoch: 0 Batch: 353 Loss: tensor(14.5832)\n","Epoch: 0 Batch: 354 Loss: tensor(14.5600)\n","Epoch: 0 Batch: 355 Loss: tensor(14.5367)\n","Epoch: 0 Batch: 356 Loss: tensor(14.5139)\n","Epoch: 0 Batch: 357 Loss: tensor(14.4905)\n","Epoch: 0 Batch: 358 Loss: tensor(14.4674)\n","Epoch: 0 Batch: 359 Loss: tensor(14.4448)\n","Epoch: 0 Batch: 360 Loss: tensor(14.4216)\n","Epoch: 0 Batch: 361 Loss: tensor(14.3985)\n","Epoch: 0 Batch: 362 Loss: tensor(14.3757)\n","Epoch: 0 Batch: 363 Loss: tensor(14.3528)\n","Epoch: 0 Batch: 364 Loss: tensor(14.3298)\n","Epoch: 0 Batch: 365 Loss: tensor(14.3070)\n","Epoch: 0 Batch: 366 Loss: tensor(14.2847)\n","Epoch: 0 Batch: 367 Loss: tensor(14.2625)\n","Epoch: 0 Batch: 368 Loss: tensor(14.2389)\n","Epoch: 0 Batch: 369 Loss: tensor(14.2161)\n","Epoch: 0 Batch: 370 Loss: tensor(14.1935)\n","Epoch: 0 Batch: 371 Loss: tensor(14.1709)\n","Epoch: 0 Batch: 372 Loss: tensor(14.1482)\n","Epoch: 0 Batch: 373 Loss: tensor(14.1262)\n","Epoch: 0 Batch: 374 Loss: tensor(14.1032)\n","Epoch: 0 Batch: 375 Loss: tensor(14.0808)\n","Epoch: 0 Batch: 376 Loss: tensor(14.0584)\n","Epoch: 0 Batch: 377 Loss: tensor(14.0360)\n","Epoch: 0 Batch: 378 Loss: tensor(14.0135)\n","Epoch: 0 Batch: 379 Loss: tensor(13.9913)\n","Epoch: 0 Batch: 380 Loss: tensor(13.9689)\n","Epoch: 0 Batch: 381 Loss: tensor(13.9468)\n","Epoch: 0 Batch: 382 Loss: tensor(13.9245)\n","Epoch: 0 Batch: 383 Loss: tensor(13.9023)\n","Epoch: 0 Batch: 384 Loss: tensor(13.8800)\n","Epoch: 0 Batch: 385 Loss: tensor(13.8580)\n","Epoch: 0 Batch: 386 Loss: tensor(13.8359)\n","Epoch: 0 Batch: 387 Loss: tensor(13.8138)\n","Epoch: 0 Batch: 388 Loss: tensor(13.7921)\n","Epoch: 0 Batch: 389 Loss: tensor(13.7699)\n","Epoch: 0 Batch: 390 Loss: tensor(13.7479)\n","Epoch: 0 Batch: 391 Loss: tensor(13.7259)\n","Epoch: 0 Batch: 392 Loss: tensor(13.7042)\n","Epoch: 0 Batch: 393 Loss: tensor(13.6823)\n","Epoch: 0 Batch: 394 Loss: tensor(13.6605)\n","Epoch: 0 Batch: 395 Loss: tensor(13.6387)\n","Epoch: 0 Batch: 396 Loss: tensor(13.6168)\n","Epoch: 0 Batch: 397 Loss: tensor(13.5952)\n","Epoch: 0 Batch: 398 Loss: tensor(13.5734)\n","Epoch: 0 Batch: 399 Loss: tensor(13.5518)\n","Epoch: 0 Batch: 400 Loss: tensor(13.5302)\n","Epoch: 0 Batch: 401 Loss: tensor(13.5085)\n","Epoch: 0 Batch: 402 Loss: tensor(13.4870)\n","Epoch: 0 Batch: 403 Loss: tensor(13.4655)\n","Epoch: 0 Batch: 404 Loss: tensor(13.4439)\n","Epoch: 0 Batch: 405 Loss: tensor(13.4224)\n","Epoch: 0 Batch: 406 Loss: tensor(13.4011)\n","Epoch: 0 Batch: 407 Loss: tensor(13.3799)\n","Epoch: 0 Batch: 408 Loss: tensor(13.3582)\n","Epoch: 0 Batch: 409 Loss: tensor(13.3369)\n","Epoch: 0 Batch: 410 Loss: tensor(13.3163)\n","Epoch: 0 Batch: 411 Loss: tensor(13.2942)\n","Epoch: 0 Batch: 412 Loss: tensor(13.2730)\n","Epoch: 0 Batch: 413 Loss: tensor(13.2519)\n","Epoch: 0 Batch: 414 Loss: tensor(13.2307)\n","Epoch: 0 Batch: 415 Loss: tensor(13.2096)\n","Epoch: 0 Batch: 416 Loss: tensor(13.1884)\n","Epoch: 0 Batch: 417 Loss: tensor(13.1674)\n","Epoch: 0 Batch: 418 Loss: tensor(13.1465)\n","Epoch: 0 Batch: 419 Loss: tensor(13.1253)\n","Epoch: 0 Batch: 420 Loss: tensor(13.1043)\n","Epoch: 0 Batch: 421 Loss: tensor(13.0836)\n","Epoch: 0 Batch: 422 Loss: tensor(13.0625)\n","Epoch: 0 Batch: 423 Loss: tensor(13.0416)\n","Epoch: 0 Batch: 424 Loss: tensor(13.0208)\n","Epoch: 0 Batch: 425 Loss: tensor(12.9999)\n","Epoch: 0 Batch: 426 Loss: tensor(12.9792)\n","Epoch: 0 Batch: 427 Loss: tensor(12.9585)\n","Epoch: 0 Batch: 428 Loss: tensor(12.9379)\n","Epoch: 0 Batch: 429 Loss: tensor(12.9170)\n","Epoch: 0 Batch: 430 Loss: tensor(12.8964)\n","Epoch: 0 Batch: 431 Loss: tensor(12.8757)\n","Epoch: 0 Batch: 432 Loss: tensor(12.8551)\n","Epoch: 0 Batch: 433 Loss: tensor(12.8347)\n","Epoch: 0 Batch: 434 Loss: tensor(12.8139)\n","Epoch: 0 Batch: 435 Loss: tensor(12.7938)\n","Epoch: 0 Batch: 436 Loss: tensor(12.7731)\n","Epoch: 0 Batch: 437 Loss: tensor(12.7525)\n","Epoch: 0 Batch: 438 Loss: tensor(12.7322)\n","Epoch: 0 Batch: 439 Loss: tensor(12.7117)\n","Epoch: 0 Batch: 440 Loss: tensor(12.6914)\n","Epoch: 0 Batch: 441 Loss: tensor(12.6717)\n","Epoch: 0 Batch: 442 Loss: tensor(12.6508)\n","Epoch: 0 Batch: 443 Loss: tensor(12.6309)\n","Epoch: 0 Batch: 444 Loss: tensor(12.6104)\n","Epoch: 0 Batch: 445 Loss: tensor(12.5901)\n","Epoch: 0 Batch: 446 Loss: tensor(12.5703)\n","Epoch: 0 Batch: 447 Loss: tensor(12.5498)\n","Epoch: 0 Batch: 448 Loss: tensor(12.5298)\n","Epoch: 0 Batch: 449 Loss: tensor(12.5097)\n","Epoch: 0 Batch: 450 Loss: tensor(12.4896)\n","Epoch: 0 Batch: 451 Loss: tensor(12.4697)\n","Epoch: 0 Batch: 452 Loss: tensor(12.4498)\n","Epoch: 0 Batch: 453 Loss: tensor(12.4297)\n","Epoch: 0 Batch: 454 Loss: tensor(12.4099)\n","Epoch: 0 Batch: 455 Loss: tensor(12.3898)\n","Epoch: 0 Batch: 456 Loss: tensor(12.3701)\n","Epoch: 0 Batch: 457 Loss: tensor(12.3501)\n","Epoch: 0 Batch: 458 Loss: tensor(12.3303)\n","Epoch: 0 Batch: 459 Loss: tensor(12.3108)\n","Epoch: 0 Batch: 460 Loss: tensor(12.2908)\n","Epoch: 0 Batch: 461 Loss: tensor(12.2712)\n","Epoch: 0 Batch: 462 Loss: tensor(12.2514)\n","Epoch: 0 Batch: 463 Loss: tensor(12.2318)\n","Epoch: 0 Batch: 464 Loss: tensor(12.2121)\n","Epoch: 0 Batch: 465 Loss: tensor(12.1927)\n","Epoch: 0 Batch: 466 Loss: tensor(12.1731)\n","Epoch: 0 Batch: 467 Loss: tensor(12.1535)\n","Epoch: 0 Batch: 468 Loss: tensor(12.1340)\n","Epoch: 0 Batch: 469 Loss: tensor(12.1148)\n","Epoch: 0 Batch: 470 Loss: tensor(12.0951)\n","Epoch: 0 Batch: 471 Loss: tensor(12.0759)\n","Epoch: 0 Batch: 472 Loss: tensor(12.0563)\n","Epoch: 0 Batch: 473 Loss: tensor(12.0368)\n","Epoch: 0 Batch: 474 Loss: tensor(12.0177)\n","Epoch: 0 Batch: 475 Loss: tensor(11.9983)\n","Epoch: 0 Batch: 476 Loss: tensor(11.9791)\n","Epoch: 0 Batch: 477 Loss: tensor(11.9599)\n","Epoch: 0 Batch: 478 Loss: tensor(11.9406)\n","Epoch: 0 Batch: 479 Loss: tensor(11.9217)\n","Epoch: 0 Batch: 480 Loss: tensor(11.9024)\n","Epoch: 0 Batch: 481 Loss: tensor(11.8832)\n","Epoch: 0 Batch: 482 Loss: tensor(11.8642)\n","Epoch: 0 Batch: 483 Loss: tensor(11.8452)\n","Epoch: 0 Batch: 484 Loss: tensor(11.8261)\n","Epoch: 0 Batch: 485 Loss: tensor(11.8071)\n","Epoch: 0 Batch: 486 Loss: tensor(11.7881)\n","Epoch: 0 Batch: 487 Loss: tensor(11.7691)\n","Epoch: 0 Batch: 488 Loss: tensor(11.7502)\n","Epoch: 0 Batch: 489 Loss: tensor(11.7313)\n","Epoch: 0 Batch: 490 Loss: tensor(11.7125)\n","Epoch: 0 Batch: 491 Loss: tensor(11.6937)\n","Epoch: 0 Batch: 492 Loss: tensor(11.6748)\n","Epoch: 0 Batch: 493 Loss: tensor(11.6561)\n","Epoch: 0 Batch: 494 Loss: tensor(11.6373)\n","Epoch: 0 Batch: 495 Loss: tensor(11.6186)\n","Epoch: 0 Batch: 496 Loss: tensor(11.6001)\n","Epoch: 0 Batch: 497 Loss: tensor(11.5817)\n","Epoch: 0 Batch: 498 Loss: tensor(11.5627)\n","Epoch: 0 Batch: 499 Loss: tensor(11.5441)\n","Epoch: 0 Batch: 500 Loss: tensor(11.5255)\n","Epoch: 0 Batch: 501 Loss: tensor(11.5069)\n","Epoch: 0 Batch: 502 Loss: tensor(11.4884)\n","Epoch: 0 Batch: 503 Loss: tensor(11.4704)\n","Epoch: 0 Batch: 504 Loss: tensor(11.4515)\n","Epoch: 0 Batch: 505 Loss: tensor(11.4332)\n","Epoch: 0 Batch: 506 Loss: tensor(11.4147)\n","Epoch: 0 Batch: 507 Loss: tensor(11.3964)\n","Epoch: 0 Batch: 508 Loss: tensor(11.3779)\n","Epoch: 0 Batch: 509 Loss: tensor(11.3596)\n","Epoch: 0 Batch: 510 Loss: tensor(11.3414)\n","Epoch: 0 Batch: 511 Loss: tensor(11.3231)\n","Epoch: 0 Batch: 512 Loss: tensor(11.3048)\n","Epoch: 0 Batch: 513 Loss: tensor(11.2866)\n","Epoch: 0 Batch: 514 Loss: tensor(11.2684)\n","Epoch: 0 Batch: 515 Loss: tensor(11.2503)\n","Epoch: 0 Batch: 516 Loss: tensor(11.2323)\n","Epoch: 0 Batch: 517 Loss: tensor(11.2140)\n","Epoch: 0 Batch: 518 Loss: tensor(11.1960)\n","Epoch: 0 Batch: 519 Loss: tensor(11.1780)\n","Epoch: 0 Batch: 520 Loss: tensor(11.1599)\n","Epoch: 0 Batch: 521 Loss: tensor(11.1420)\n","Epoch: 0 Batch: 522 Loss: tensor(11.1240)\n","Epoch: 0 Batch: 523 Loss: tensor(11.1061)\n","Epoch: 0 Batch: 524 Loss: tensor(11.0882)\n","Epoch: 0 Batch: 525 Loss: tensor(11.0704)\n","Epoch: 0 Batch: 526 Loss: tensor(11.0526)\n","Epoch: 0 Batch: 527 Loss: tensor(11.0349)\n","Epoch: 0 Batch: 528 Loss: tensor(11.0169)\n","Epoch: 0 Batch: 529 Loss: tensor(10.9992)\n","Epoch: 0 Batch: 530 Loss: tensor(10.9814)\n","Epoch: 0 Batch: 531 Loss: tensor(10.9637)\n","Epoch: 0 Batch: 532 Loss: tensor(10.9460)\n","Epoch: 0 Batch: 533 Loss: tensor(10.9284)\n","Epoch: 0 Batch: 534 Loss: tensor(10.9107)\n","Epoch: 0 Batch: 535 Loss: tensor(10.8931)\n","Epoch: 0 Batch: 536 Loss: tensor(10.8756)\n","Epoch: 0 Batch: 537 Loss: tensor(10.8580)\n","Epoch: 0 Batch: 538 Loss: tensor(10.8405)\n","Epoch: 0 Batch: 539 Loss: tensor(10.8231)\n","Epoch: 0 Batch: 540 Loss: tensor(10.8057)\n","Epoch: 0 Batch: 541 Loss: tensor(10.7881)\n","Epoch: 0 Batch: 542 Loss: tensor(10.7706)\n","Epoch: 0 Batch: 543 Loss: tensor(10.7533)\n","Epoch: 0 Batch: 544 Loss: tensor(10.7360)\n","Epoch: 0 Batch: 545 Loss: tensor(10.7186)\n","Epoch: 0 Batch: 546 Loss: tensor(10.7013)\n","Epoch: 0 Batch: 547 Loss: tensor(10.6840)\n","Epoch: 0 Batch: 548 Loss: tensor(10.6667)\n","Epoch: 0 Batch: 549 Loss: tensor(10.6499)\n","Epoch: 0 Batch: 550 Loss: tensor(10.6324)\n","Epoch: 0 Batch: 551 Loss: tensor(10.6152)\n","Epoch: 0 Batch: 552 Loss: tensor(10.5981)\n","Epoch: 0 Batch: 553 Loss: tensor(10.5810)\n","Epoch: 0 Batch: 554 Loss: tensor(10.5638)\n","Epoch: 0 Batch: 555 Loss: tensor(10.5467)\n","Epoch: 0 Batch: 556 Loss: tensor(10.5298)\n","Epoch: 0 Batch: 557 Loss: tensor(10.5126)\n","Epoch: 0 Batch: 558 Loss: tensor(10.4957)\n","Epoch: 0 Batch: 559 Loss: tensor(10.4788)\n","Epoch: 0 Batch: 560 Loss: tensor(10.4617)\n","Epoch: 0 Batch: 561 Loss: tensor(10.4447)\n","Epoch: 0 Batch: 562 Loss: tensor(10.4279)\n","Epoch: 0 Batch: 563 Loss: tensor(10.4110)\n","Epoch: 0 Batch: 564 Loss: tensor(10.3942)\n","Epoch: 0 Batch: 565 Loss: tensor(10.3773)\n","Epoch: 0 Batch: 566 Loss: tensor(10.3606)\n","Epoch: 0 Batch: 567 Loss: tensor(10.3439)\n","Epoch: 0 Batch: 568 Loss: tensor(10.3272)\n","Epoch: 0 Batch: 569 Loss: tensor(10.3104)\n","Epoch: 0 Batch: 570 Loss: tensor(10.2937)\n","Epoch: 0 Batch: 571 Loss: tensor(10.2770)\n","Epoch: 0 Batch: 572 Loss: tensor(10.2604)\n","Epoch: 0 Batch: 573 Loss: tensor(10.2439)\n","Epoch: 0 Batch: 574 Loss: tensor(10.2273)\n","Epoch: 0 Batch: 575 Loss: tensor(10.2106)\n","Epoch: 0 Batch: 576 Loss: tensor(10.1941)\n","Epoch: 0 Batch: 577 Loss: tensor(10.1777)\n","Epoch: 0 Batch: 578 Loss: tensor(10.1612)\n","Epoch: 0 Batch: 579 Loss: tensor(10.1448)\n","Epoch: 0 Batch: 580 Loss: tensor(10.1283)\n","Epoch: 0 Batch: 581 Loss: tensor(10.1119)\n","Epoch: 0 Batch: 582 Loss: tensor(10.0955)\n","Epoch: 0 Batch: 583 Loss: tensor(10.0792)\n","Epoch: 0 Batch: 584 Loss: tensor(10.0629)\n","Epoch: 0 Batch: 585 Loss: tensor(10.0465)\n","Epoch: 0 Batch: 586 Loss: tensor(10.0304)\n","Epoch: 0 Batch: 587 Loss: tensor(10.0139)\n","Epoch: 0 Batch: 588 Loss: tensor(9.9978)\n","Epoch: 0 Batch: 589 Loss: tensor(9.9816)\n","Epoch: 0 Batch: 590 Loss: tensor(9.9654)\n","Epoch: 0 Batch: 591 Loss: tensor(9.9494)\n","Epoch: 0 Batch: 592 Loss: tensor(9.9331)\n","Epoch: 0 Batch: 593 Loss: tensor(9.9171)\n","Epoch: 0 Batch: 594 Loss: tensor(9.9010)\n","Epoch: 0 Batch: 595 Loss: tensor(9.8850)\n","Epoch: 0 Batch: 596 Loss: tensor(9.8689)\n","Epoch: 0 Batch: 597 Loss: tensor(9.8529)\n","Epoch: 0 Batch: 598 Loss: tensor(9.8369)\n","Epoch: 0 Batch: 599 Loss: tensor(9.8209)\n","Epoch: 0 Batch: 600 Loss: tensor(9.8050)\n","Epoch: 0 Batch: 601 Loss: tensor(9.7891)\n","Epoch: 0 Batch: 602 Loss: tensor(9.7732)\n","Epoch: 0 Batch: 603 Loss: tensor(9.7573)\n","Epoch: 0 Batch: 604 Loss: tensor(9.7414)\n","Epoch: 0 Batch: 605 Loss: tensor(9.7256)\n","Epoch: 0 Batch: 606 Loss: tensor(9.7099)\n","Epoch: 0 Batch: 607 Loss: tensor(9.6941)\n","Epoch: 0 Batch: 608 Loss: tensor(9.6787)\n","Epoch: 0 Batch: 609 Loss: tensor(9.6628)\n","Epoch: 0 Batch: 610 Loss: tensor(9.6470)\n","Epoch: 0 Batch: 611 Loss: tensor(9.6313)\n","Epoch: 0 Batch: 612 Loss: tensor(9.6157)\n","Epoch: 0 Batch: 613 Loss: tensor(9.6001)\n","Epoch: 0 Batch: 614 Loss: tensor(9.5846)\n","Epoch: 0 Batch: 615 Loss: tensor(9.5690)\n","Epoch: 0 Batch: 616 Loss: tensor(9.5534)\n","Epoch: 0 Batch: 617 Loss: tensor(9.5379)\n","Epoch: 0 Batch: 618 Loss: tensor(9.5224)\n","Epoch: 0 Batch: 619 Loss: tensor(9.5070)\n","Epoch: 0 Batch: 620 Loss: tensor(9.4915)\n","Epoch: 0 Batch: 621 Loss: tensor(9.4760)\n","Epoch: 0 Batch: 622 Loss: tensor(9.4606)\n","Epoch: 0 Batch: 623 Loss: tensor(9.4453)\n","Epoch: 0 Batch: 624 Loss: tensor(9.4298)\n","Epoch: 0 Batch: 625 Loss: tensor(9.4146)\n","Epoch: 0 Batch: 626 Loss: tensor(9.3993)\n","Epoch: 0 Batch: 627 Loss: tensor(9.3840)\n","Epoch: 0 Batch: 628 Loss: tensor(9.3688)\n","Epoch: 0 Batch: 629 Loss: tensor(9.3535)\n","Epoch: 0 Batch: 630 Loss: tensor(9.3383)\n","Epoch: 0 Batch: 631 Loss: tensor(9.3231)\n","Epoch: 0 Batch: 632 Loss: tensor(9.3079)\n","Epoch: 0 Batch: 633 Loss: tensor(9.2927)\n","Epoch: 0 Batch: 634 Loss: tensor(9.2777)\n","Epoch: 0 Batch: 635 Loss: tensor(9.2627)\n","Epoch: 0 Batch: 636 Loss: tensor(9.2475)\n","Epoch: 0 Batch: 637 Loss: tensor(9.2324)\n","Epoch: 0 Batch: 638 Loss: tensor(9.2174)\n","Epoch: 0 Batch: 639 Loss: tensor(9.2024)\n","Epoch: 0 Batch: 640 Loss: tensor(9.1875)\n","Epoch: 0 Batch: 641 Loss: tensor(9.1725)\n","Epoch: 0 Batch: 642 Loss: tensor(9.1577)\n","Epoch: 0 Batch: 643 Loss: tensor(9.1428)\n","Epoch: 0 Batch: 644 Loss: tensor(9.1277)\n","Epoch: 0 Batch: 645 Loss: tensor(9.1128)\n","Epoch: 0 Batch: 646 Loss: tensor(9.0981)\n","Epoch: 0 Batch: 647 Loss: tensor(9.0832)\n","Epoch: 0 Batch: 648 Loss: tensor(9.0684)\n","Epoch: 0 Batch: 649 Loss: tensor(9.0536)\n","Epoch: 0 Batch: 650 Loss: tensor(9.0389)\n","Epoch: 0 Batch: 651 Loss: tensor(9.0242)\n","Epoch: 0 Batch: 652 Loss: tensor(9.0094)\n","Epoch: 0 Batch: 653 Loss: tensor(8.9947)\n","Epoch: 0 Batch: 654 Loss: tensor(8.9802)\n","Epoch: 0 Batch: 655 Loss: tensor(8.9656)\n","Epoch: 0 Batch: 656 Loss: tensor(8.9508)\n","Epoch: 0 Batch: 657 Loss: tensor(8.9363)\n","Epoch: 0 Batch: 658 Loss: tensor(8.9216)\n","Epoch: 0 Batch: 659 Loss: tensor(8.9071)\n","Epoch: 0 Batch: 660 Loss: tensor(8.8926)\n","Epoch: 0 Batch: 661 Loss: tensor(8.8781)\n","Epoch: 0 Batch: 662 Loss: tensor(8.8636)\n","Epoch: 0 Batch: 663 Loss: tensor(8.8492)\n","Epoch: 0 Batch: 664 Loss: tensor(8.8347)\n","Epoch: 0 Batch: 665 Loss: tensor(8.8203)\n","Epoch: 0 Batch: 666 Loss: tensor(8.8060)\n","Epoch: 0 Batch: 667 Loss: tensor(8.7915)\n","Epoch: 0 Batch: 668 Loss: tensor(8.7772)\n","Epoch: 0 Batch: 669 Loss: tensor(8.7629)\n","Epoch: 0 Batch: 670 Loss: tensor(8.7487)\n","Epoch: 0 Batch: 671 Loss: tensor(8.7343)\n","Epoch: 0 Batch: 672 Loss: tensor(8.7200)\n","Epoch: 0 Batch: 673 Loss: tensor(8.7058)\n","Epoch: 0 Batch: 674 Loss: tensor(8.6916)\n","Epoch: 0 Batch: 675 Loss: tensor(8.6775)\n","Epoch: 0 Batch: 676 Loss: tensor(8.6632)\n","Epoch: 0 Batch: 677 Loss: tensor(8.6491)\n","Epoch: 0 Batch: 678 Loss: tensor(8.6350)\n","Epoch: 0 Batch: 679 Loss: tensor(8.6209)\n","Epoch: 0 Batch: 680 Loss: tensor(8.6068)\n","Epoch: 0 Batch: 681 Loss: tensor(8.5927)\n","Epoch: 0 Batch: 682 Loss: tensor(8.5787)\n","Epoch: 0 Batch: 683 Loss: tensor(8.5646)\n","Epoch: 0 Batch: 684 Loss: tensor(8.5506)\n","Epoch: 0 Batch: 685 Loss: tensor(8.5366)\n","Epoch: 0 Batch: 686 Loss: tensor(8.5228)\n","Epoch: 0 Batch: 687 Loss: tensor(8.5088)\n","Epoch: 0 Batch: 688 Loss: tensor(8.4949)\n","Epoch: 0 Batch: 689 Loss: tensor(8.4811)\n","Epoch: 0 Batch: 690 Loss: tensor(8.4671)\n","Epoch: 0 Batch: 691 Loss: tensor(8.4533)\n","Epoch: 0 Batch: 692 Loss: tensor(8.4395)\n","Epoch: 0 Batch: 693 Loss: tensor(8.4256)\n","Epoch: 0 Batch: 694 Loss: tensor(8.4119)\n","Epoch: 0 Batch: 695 Loss: tensor(8.3981)\n","Epoch: 0 Batch: 696 Loss: tensor(8.3844)\n","Epoch: 0 Batch: 697 Loss: tensor(8.3706)\n","Epoch: 0 Batch: 698 Loss: tensor(8.3569)\n","Epoch: 0 Batch: 699 Loss: tensor(8.3433)\n","Epoch: 0 Batch: 700 Loss: tensor(8.3297)\n","Epoch: 0 Batch: 701 Loss: tensor(8.3160)\n","Epoch: 0 Batch: 702 Loss: tensor(8.3023)\n","Epoch: 0 Batch: 703 Loss: tensor(8.2888)\n","Epoch: 0 Batch: 704 Loss: tensor(8.2752)\n","Epoch: 0 Batch: 705 Loss: tensor(8.2616)\n","Epoch: 0 Batch: 706 Loss: tensor(8.2481)\n","Epoch: 0 Batch: 707 Loss: tensor(8.2346)\n","Epoch: 0 Batch: 708 Loss: tensor(8.2212)\n","Epoch: 0 Batch: 709 Loss: tensor(8.2077)\n","Epoch: 0 Batch: 710 Loss: tensor(8.1942)\n","Epoch: 0 Batch: 711 Loss: tensor(8.1808)\n","Epoch: 0 Batch: 712 Loss: tensor(8.1676)\n","Epoch: 0 Batch: 713 Loss: tensor(8.1540)\n","Epoch: 0 Batch: 714 Loss: tensor(8.1406)\n","Epoch: 0 Batch: 715 Loss: tensor(8.1273)\n","Epoch: 0 Batch: 716 Loss: tensor(8.1140)\n","Epoch: 0 Batch: 717 Loss: tensor(8.1007)\n","Epoch: 0 Batch: 718 Loss: tensor(8.0876)\n","Epoch: 0 Batch: 719 Loss: tensor(8.0741)\n","Epoch: 0 Batch: 720 Loss: tensor(8.0609)\n","Epoch: 0 Batch: 721 Loss: tensor(8.0476)\n","Epoch: 0 Batch: 722 Loss: tensor(8.0345)\n","Epoch: 0 Batch: 723 Loss: tensor(8.0213)\n","Epoch: 0 Batch: 724 Loss: tensor(8.0081)\n","Epoch: 0 Batch: 725 Loss: tensor(7.9950)\n","Epoch: 0 Batch: 726 Loss: tensor(7.9819)\n","Epoch: 0 Batch: 727 Loss: tensor(7.9687)\n","Epoch: 0 Batch: 728 Loss: tensor(7.9557)\n","Epoch: 0 Batch: 729 Loss: tensor(7.9426)\n","Epoch: 0 Batch: 730 Loss: tensor(7.9296)\n","Epoch: 0 Batch: 731 Loss: tensor(7.9166)\n","Epoch: 0 Batch: 732 Loss: tensor(7.9036)\n","Epoch: 0 Batch: 733 Loss: tensor(7.8906)\n","Epoch: 0 Batch: 734 Loss: tensor(7.8777)\n","Epoch: 0 Batch: 735 Loss: tensor(7.8647)\n","Epoch: 0 Batch: 736 Loss: tensor(7.8520)\n","Epoch: 0 Batch: 737 Loss: tensor(7.8389)\n","Epoch: 0 Batch: 738 Loss: tensor(7.8261)\n","Epoch: 0 Batch: 739 Loss: tensor(7.8131)\n","Epoch: 0 Batch: 740 Loss: tensor(7.8003)\n","Epoch: 0 Batch: 741 Loss: tensor(7.7877)\n","Epoch: 0 Batch: 742 Loss: tensor(7.7747)\n","Epoch: 0 Batch: 743 Loss: tensor(7.7619)\n","Epoch: 0 Batch: 744 Loss: tensor(7.7492)\n","Epoch: 0 Batch: 745 Loss: tensor(7.7365)\n","Epoch: 0 Batch: 746 Loss: tensor(7.7237)\n","Epoch: 0 Batch: 747 Loss: tensor(7.7111)\n","Epoch: 0 Batch: 748 Loss: tensor(7.6984)\n","Epoch: 0 Batch: 749 Loss: tensor(7.6858)\n","Epoch: 0 Batch: 750 Loss: tensor(7.6730)\n","Epoch: 0 Batch: 751 Loss: tensor(7.6604)\n","Epoch: 0 Batch: 752 Loss: tensor(7.6480)\n","Epoch: 0 Batch: 753 Loss: tensor(7.6353)\n","Epoch: 0 Batch: 754 Loss: tensor(7.6227)\n","Epoch: 0 Batch: 755 Loss: tensor(7.6102)\n","Epoch: 0 Batch: 756 Loss: tensor(7.5976)\n","Epoch: 0 Batch: 757 Loss: tensor(7.5854)\n","Epoch: 0 Batch: 758 Loss: tensor(7.5727)\n","Epoch: 0 Batch: 759 Loss: tensor(7.5602)\n","Epoch: 0 Batch: 760 Loss: tensor(7.5477)\n","Epoch: 0 Batch: 761 Loss: tensor(7.5353)\n","Epoch: 0 Batch: 762 Loss: tensor(7.5229)\n","Epoch: 0 Batch: 763 Loss: tensor(7.5105)\n","Epoch: 0 Batch: 764 Loss: tensor(7.4981)\n","Epoch: 0 Batch: 765 Loss: tensor(7.4858)\n","Epoch: 0 Batch: 766 Loss: tensor(7.4735)\n","Epoch: 0 Batch: 767 Loss: tensor(7.4613)\n","Epoch: 0 Batch: 768 Loss: tensor(7.4488)\n","Epoch: 0 Batch: 769 Loss: tensor(7.4367)\n","Epoch: 0 Batch: 770 Loss: tensor(7.4244)\n","Epoch: 0 Batch: 771 Loss: tensor(7.4121)\n","Epoch: 0 Batch: 772 Loss: tensor(7.3999)\n","Epoch: 0 Batch: 773 Loss: tensor(7.3877)\n","Epoch: 0 Batch: 774 Loss: tensor(7.3756)\n","Epoch: 0 Batch: 775 Loss: tensor(7.3635)\n","Epoch: 0 Batch: 776 Loss: tensor(7.3512)\n","Epoch: 0 Batch: 777 Loss: tensor(7.3391)\n","Epoch: 0 Batch: 778 Loss: tensor(7.3271)\n","Epoch: 0 Batch: 779 Loss: tensor(7.3150)\n","Epoch: 0 Batch: 780 Loss: tensor(7.3029)\n","Epoch: 0 Batch: 781 Loss: tensor(7.2910)\n","Epoch: 0 Batch: 782 Loss: tensor(7.2788)\n","Epoch: 0 Batch: 783 Loss: tensor(7.2669)\n","Epoch: 0 Batch: 784 Loss: tensor(7.2548)\n","Epoch: 0 Batch: 785 Loss: tensor(7.2428)\n","Epoch: 0 Batch: 786 Loss: tensor(7.2310)\n","Epoch: 0 Batch: 787 Loss: tensor(7.2190)\n","Epoch: 0 Batch: 788 Loss: tensor(7.2071)\n","Epoch: 0 Batch: 789 Loss: tensor(7.1952)\n","Epoch: 0 Batch: 790 Loss: tensor(7.1833)\n","Epoch: 0 Batch: 791 Loss: tensor(7.1715)\n","Epoch: 0 Batch: 792 Loss: tensor(7.1597)\n","Epoch: 0 Batch: 793 Loss: tensor(7.1478)\n","Epoch: 0 Batch: 794 Loss: tensor(7.1360)\n","Epoch: 0 Batch: 795 Loss: tensor(7.1242)\n","Epoch: 0 Batch: 796 Loss: tensor(7.1124)\n","Epoch: 0 Batch: 797 Loss: tensor(7.1007)\n","Epoch: 0 Batch: 798 Loss: tensor(7.0890)\n","Epoch: 0 Batch: 799 Loss: tensor(7.0773)\n","Epoch: 0 Batch: 800 Loss: tensor(7.0655)\n","Epoch: 0 Batch: 801 Loss: tensor(7.0538)\n","Epoch: 0 Batch: 802 Loss: tensor(7.0422)\n","Epoch: 0 Batch: 803 Loss: tensor(7.0307)\n","Epoch: 0 Batch: 804 Loss: tensor(7.0189)\n","Epoch: 0 Batch: 805 Loss: tensor(7.0073)\n","Epoch: 0 Batch: 806 Loss: tensor(6.9958)\n","Epoch: 0 Batch: 807 Loss: tensor(6.9841)\n","Epoch: 0 Batch: 808 Loss: tensor(6.9726)\n","Epoch: 0 Batch: 809 Loss: tensor(6.9611)\n","Epoch: 0 Batch: 810 Loss: tensor(6.9496)\n","Epoch: 0 Batch: 811 Loss: tensor(6.9382)\n","Epoch: 0 Batch: 812 Loss: tensor(6.9267)\n","Epoch: 0 Batch: 813 Loss: tensor(6.9151)\n","Epoch: 0 Batch: 814 Loss: tensor(6.9037)\n","Epoch: 0 Batch: 815 Loss: tensor(6.8923)\n","Epoch: 0 Batch: 816 Loss: tensor(6.8809)\n","Epoch: 0 Batch: 817 Loss: tensor(6.8695)\n","Epoch: 0 Batch: 818 Loss: tensor(6.8582)\n","Epoch: 0 Batch: 819 Loss: tensor(6.8467)\n","Epoch: 0 Batch: 820 Loss: tensor(6.8356)\n","Epoch: 0 Batch: 821 Loss: tensor(6.8241)\n","Epoch: 0 Batch: 822 Loss: tensor(6.8128)\n","Epoch: 0 Batch: 823 Loss: tensor(6.8016)\n","Epoch: 0 Batch: 824 Loss: tensor(6.7903)\n","Epoch: 0 Batch: 825 Loss: tensor(6.7791)\n","Epoch: 0 Batch: 826 Loss: tensor(6.7681)\n","Epoch: 0 Batch: 827 Loss: tensor(6.7566)\n","Epoch: 0 Batch: 828 Loss: tensor(6.7454)\n","Epoch: 0 Batch: 829 Loss: tensor(6.7343)\n","Epoch: 0 Batch: 830 Loss: tensor(6.7231)\n","Epoch: 0 Batch: 831 Loss: tensor(6.7119)\n","Epoch: 0 Batch: 832 Loss: tensor(6.7008)\n","Epoch: 0 Batch: 833 Loss: tensor(6.6897)\n","Epoch: 0 Batch: 834 Loss: tensor(6.6786)\n","Epoch: 0 Batch: 835 Loss: tensor(6.6675)\n","Epoch: 0 Batch: 836 Loss: tensor(6.6565)\n","Epoch: 0 Batch: 837 Loss: tensor(6.6454)\n","Epoch: 0 Batch: 838 Loss: tensor(6.6344)\n","Epoch: 0 Batch: 839 Loss: tensor(6.6235)\n","Epoch: 0 Batch: 840 Loss: tensor(6.6126)\n","Epoch: 0 Batch: 841 Loss: tensor(6.6015)\n","Epoch: 0 Batch: 842 Loss: tensor(6.5907)\n","Epoch: 0 Batch: 843 Loss: tensor(6.5796)\n","Epoch: 0 Batch: 844 Loss: tensor(6.5686)\n","Epoch: 0 Batch: 845 Loss: tensor(6.5578)\n","Epoch: 0 Batch: 846 Loss: tensor(6.5469)\n","Epoch: 0 Batch: 847 Loss: tensor(6.5360)\n","Epoch: 0 Batch: 848 Loss: tensor(6.5252)\n","Epoch: 0 Batch: 849 Loss: tensor(6.5143)\n","Epoch: 0 Batch: 850 Loss: tensor(6.5036)\n","Epoch: 0 Batch: 851 Loss: tensor(6.4926)\n","Epoch: 0 Batch: 852 Loss: tensor(6.4819)\n","Epoch: 0 Batch: 853 Loss: tensor(6.4711)\n","Epoch: 0 Batch: 854 Loss: tensor(6.4604)\n","Epoch: 0 Batch: 855 Loss: tensor(6.4496)\n","Epoch: 0 Batch: 856 Loss: tensor(6.4390)\n","Epoch: 0 Batch: 857 Loss: tensor(6.4283)\n","Epoch: 0 Batch: 858 Loss: tensor(6.4176)\n","Epoch: 0 Batch: 859 Loss: tensor(6.4069)\n","Epoch: 0 Batch: 860 Loss: tensor(6.3962)\n","Epoch: 0 Batch: 861 Loss: tensor(6.3856)\n","Epoch: 0 Batch: 862 Loss: tensor(6.3750)\n","Epoch: 0 Batch: 863 Loss: tensor(6.3644)\n","Epoch: 0 Batch: 864 Loss: tensor(6.3538)\n","Epoch: 0 Batch: 865 Loss: tensor(6.3432)\n","Epoch: 0 Batch: 866 Loss: tensor(6.3328)\n","Epoch: 0 Batch: 867 Loss: tensor(6.3222)\n","Epoch: 0 Batch: 868 Loss: tensor(6.3118)\n","Epoch: 0 Batch: 869 Loss: tensor(6.3012)\n","Epoch: 0 Batch: 870 Loss: tensor(6.2907)\n","Epoch: 0 Batch: 871 Loss: tensor(6.2802)\n","Epoch: 0 Batch: 872 Loss: tensor(6.2698)\n","Epoch: 0 Batch: 873 Loss: tensor(6.2594)\n","Epoch: 0 Batch: 874 Loss: tensor(6.2489)\n","Epoch: 0 Batch: 875 Loss: tensor(6.2386)\n","Epoch: 0 Batch: 876 Loss: tensor(6.2282)\n","Epoch: 0 Batch: 877 Loss: tensor(6.2178)\n","Epoch: 0 Batch: 878 Loss: tensor(6.2074)\n","Epoch: 0 Batch: 879 Loss: tensor(6.1971)\n","Epoch: 0 Batch: 880 Loss: tensor(6.1868)\n","Epoch: 0 Batch: 881 Loss: tensor(6.1765)\n","Epoch: 0 Batch: 882 Loss: tensor(6.1662)\n","Epoch: 0 Batch: 883 Loss: tensor(6.1559)\n","Epoch: 0 Batch: 884 Loss: tensor(6.1456)\n","Epoch: 0 Batch: 885 Loss: tensor(6.1354)\n","Epoch: 0 Batch: 886 Loss: tensor(6.1252)\n","Epoch: 0 Batch: 887 Loss: tensor(6.1150)\n","Epoch: 0 Batch: 888 Loss: tensor(6.1048)\n","Epoch: 0 Batch: 889 Loss: tensor(6.0948)\n","Epoch: 0 Batch: 890 Loss: tensor(6.0845)\n","Epoch: 0 Batch: 891 Loss: tensor(6.0743)\n","Epoch: 0 Batch: 892 Loss: tensor(6.0642)\n","Epoch: 0 Batch: 893 Loss: tensor(6.0541)\n","Epoch: 0 Batch: 894 Loss: tensor(6.0440)\n","Epoch: 0 Batch: 895 Loss: tensor(6.0339)\n","Epoch: 0 Batch: 896 Loss: tensor(6.0239)\n","Epoch: 0 Batch: 897 Loss: tensor(6.0138)\n","Epoch: 0 Batch: 898 Loss: tensor(6.0038)\n","Epoch: 0 Batch: 899 Loss: tensor(5.9938)\n","Epoch: 0 Batch: 900 Loss: tensor(5.9837)\n","Epoch: 0 Batch: 901 Loss: tensor(5.9738)\n","Epoch: 0 Batch: 902 Loss: tensor(5.9638)\n","Epoch: 0 Batch: 903 Loss: tensor(5.9538)\n","Epoch: 0 Batch: 904 Loss: tensor(5.9439)\n","Epoch: 0 Batch: 905 Loss: tensor(5.9340)\n","Epoch: 0 Batch: 906 Loss: tensor(5.9241)\n","Epoch: 0 Batch: 907 Loss: tensor(5.9142)\n","Epoch: 0 Batch: 908 Loss: tensor(5.9043)\n","Epoch: 0 Batch: 909 Loss: tensor(5.8945)\n","Epoch: 0 Batch: 910 Loss: tensor(5.8846)\n","Epoch: 0 Batch: 911 Loss: tensor(5.8748)\n","Epoch: 0 Batch: 912 Loss: tensor(5.8650)\n","Epoch: 0 Batch: 913 Loss: tensor(5.8552)\n","Epoch: 0 Batch: 914 Loss: tensor(5.8454)\n","Epoch: 0 Batch: 915 Loss: tensor(5.8357)\n","Epoch: 0 Batch: 916 Loss: tensor(5.8258)\n","Epoch: 0 Batch: 917 Loss: tensor(5.8161)\n","Epoch: 0 Batch: 918 Loss: tensor(5.8064)\n","Epoch: 0 Batch: 919 Loss: tensor(5.7967)\n","Epoch: 0 Batch: 920 Loss: tensor(5.7870)\n","Epoch: 0 Batch: 921 Loss: tensor(5.7773)\n","Epoch: 0 Batch: 922 Loss: tensor(5.7677)\n","Epoch: 0 Batch: 923 Loss: tensor(5.7580)\n","Epoch: 0 Batch: 924 Loss: tensor(5.7484)\n","Epoch: 0 Batch: 925 Loss: tensor(5.7388)\n","Epoch: 0 Batch: 926 Loss: tensor(5.7291)\n","Epoch: 0 Batch: 927 Loss: tensor(5.7196)\n","Epoch: 0 Batch: 928 Loss: tensor(5.7100)\n","Epoch: 0 Batch: 929 Loss: tensor(5.7004)\n","Epoch: 0 Batch: 930 Loss: tensor(5.6909)\n","Epoch: 0 Batch: 931 Loss: tensor(5.6814)\n","Epoch: 0 Batch: 932 Loss: tensor(5.6719)\n","Epoch: 0 Batch: 933 Loss: tensor(5.6624)\n","Epoch: 0 Batch: 934 Loss: tensor(5.6529)\n","Epoch: 0 Batch: 935 Loss: tensor(5.6435)\n","Epoch: 0 Batch: 936 Loss: tensor(5.6340)\n","Epoch: 0 Batch: 937 Loss: tensor(5.6246)\n","Epoch: 0 Batch: 938 Loss: tensor(5.6151)\n","Epoch: 0 Batch: 939 Loss: tensor(5.6057)\n","Epoch: 0 Batch: 940 Loss: tensor(5.5963)\n","Epoch: 0 Batch: 941 Loss: tensor(5.5870)\n","Epoch: 0 Batch: 942 Loss: tensor(5.5776)\n","Epoch: 0 Batch: 943 Loss: tensor(5.5683)\n","Epoch: 0 Batch: 944 Loss: tensor(5.5589)\n","Epoch: 0 Batch: 945 Loss: tensor(5.5496)\n","Epoch: 0 Batch: 946 Loss: tensor(5.5403)\n","Epoch: 0 Batch: 947 Loss: tensor(5.5310)\n","Epoch: 0 Batch: 948 Loss: tensor(5.5217)\n","Epoch: 0 Batch: 949 Loss: tensor(5.5125)\n","Epoch: 0 Batch: 950 Loss: tensor(5.5033)\n","Epoch: 0 Batch: 951 Loss: tensor(5.4940)\n","Epoch: 0 Batch: 952 Loss: tensor(5.4848)\n","Epoch: 0 Batch: 953 Loss: tensor(5.4756)\n","Epoch: 0 Batch: 954 Loss: tensor(5.4664)\n","Epoch: 0 Batch: 955 Loss: tensor(5.4572)\n","Epoch: 0 Batch: 956 Loss: tensor(5.4480)\n","Epoch: 0 Batch: 957 Loss: tensor(5.4389)\n","Epoch: 0 Batch: 958 Loss: tensor(5.4298)\n","Epoch: 0 Batch: 959 Loss: tensor(5.4207)\n","Epoch: 0 Batch: 960 Loss: tensor(5.4116)\n","Epoch: 0 Batch: 961 Loss: tensor(5.4025)\n","Epoch: 0 Batch: 962 Loss: tensor(5.3935)\n","Epoch: 0 Batch: 963 Loss: tensor(5.3844)\n","Epoch: 0 Batch: 964 Loss: tensor(5.3753)\n","Epoch: 0 Batch: 965 Loss: tensor(5.3663)\n","Epoch: 0 Batch: 966 Loss: tensor(5.3573)\n","Epoch: 0 Batch: 967 Loss: tensor(5.3484)\n","Epoch: 0 Batch: 968 Loss: tensor(5.3393)\n","Epoch: 0 Batch: 969 Loss: tensor(5.3303)\n","Epoch: 0 Batch: 970 Loss: tensor(5.3214)\n","Epoch: 0 Batch: 971 Loss: tensor(5.3124)\n","Epoch: 0 Batch: 972 Loss: tensor(5.3035)\n","Epoch: 0 Batch: 973 Loss: tensor(5.2946)\n","Epoch: 0 Batch: 974 Loss: tensor(5.2857)\n","Epoch: 0 Batch: 975 Loss: tensor(5.2768)\n","Epoch: 0 Batch: 976 Loss: tensor(5.2679)\n","Epoch: 0 Batch: 977 Loss: tensor(5.2591)\n","Epoch: 0 Batch: 978 Loss: tensor(5.2502)\n","Epoch: 0 Batch: 979 Loss: tensor(5.2414)\n","Epoch: 0 Batch: 980 Loss: tensor(5.2326)\n","Epoch: 0 Batch: 981 Loss: tensor(5.2238)\n","Epoch: 0 Batch: 982 Loss: tensor(5.2150)\n","Epoch: 0 Batch: 983 Loss: tensor(5.2062)\n","Epoch: 0 Batch: 984 Loss: tensor(5.1976)\n","Epoch: 0 Batch: 985 Loss: tensor(5.1887)\n","Epoch: 0 Batch: 986 Loss: tensor(5.1800)\n","Epoch: 0 Batch: 987 Loss: tensor(5.1713)\n","Epoch: 0 Batch: 988 Loss: tensor(5.1625)\n","Epoch: 0 Batch: 989 Loss: tensor(5.1539)\n","Epoch: 0 Batch: 990 Loss: tensor(5.1452)\n","Epoch: 0 Batch: 991 Loss: tensor(5.1365)\n","Epoch: 0 Batch: 992 Loss: tensor(5.1278)\n","Epoch: 0 Batch: 993 Loss: tensor(5.1192)\n","Epoch: 0 Batch: 994 Loss: tensor(5.1106)\n","Epoch: 0 Batch: 995 Loss: tensor(5.1019)\n","Epoch: 0 Batch: 996 Loss: tensor(5.0934)\n","Epoch: 0 Batch: 997 Loss: tensor(5.0848)\n","Epoch: 0 Batch: 998 Loss: tensor(5.0762)\n","Epoch: 0 Batch: 999 Loss: tensor(5.0677)\n","[1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Epoch:  0 Bias Accuracy:  0.996\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybqEO4jrChPX","executionInfo":{"status":"ok","timestamp":1619628374073,"user_tz":-330,"elapsed":6759026,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}},"outputId":"699cc3d3-4bd3-479b-f28f-7fd8444c9137"},"source":["acc = accuracy(train_batch_acc, transfer_bias_nb_batches)\n","print(\"Epoch: \", epoch, \"Bias Accuracy: \", acc)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","Epoch:  0 Bias Accuracy:  0.996\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XlFwkng7ChSE","executionInfo":{"status":"ok","timestamp":1619629876056,"user_tz":-330,"elapsed":1264,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}},"outputId":"67acf321-058b-40e5-cdb1-9e73f134cebc"},"source":["import csv\n","file = open(\"toxic_train_30k.csv\")\n","reader = csv.reader(file)\n","lines= len(list(reader))\n","print(lines)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["30001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AXu2Hid9CZ1z","executionInfo":{"status":"ok","timestamp":1619629827169,"user_tz":-330,"elapsed":1000,"user":{"displayName":"Niharika Pentapati","photoUrl":"","userId":"13725049464969353352"}},"outputId":"919ef90b-59d3-42cb-c344-069f38db826e"},"source":["!pwd"],"execution_count":31,"outputs":[{"output_type":"stream","text":["/content/capstone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VfyXjb57CZ9U"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zn6DW-HDCaEV"},"source":[""],"execution_count":null,"outputs":[]}]}